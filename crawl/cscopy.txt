https://en.wikipedia.org/wiki/Computer_science
Computer science - Wikipedia
Computer science is the study of algorithmic processes, computational machines and computation itself. As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computational systems in hardware and software.Its fields can be divided into theoretical and practical disciplines. For example, the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them, while computer graphics or computational geometry emphasize more specific applications. Algorithms and data structures have been called the heart of computer science. Programming language theory considers approaches to the description of computational processes, while computer programming involves the use of them to create complex systems. Computer architecture describes construction of computer components and computer-operated equipment. Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. A digital computer is capable of simulating various information processes. The fundamental concern of computer science is determining what can and cannot be automated. Computer scientists usually focus on academic research. The Turing Award is generally recognized as the highest distinction in computer sciences.\n\n\n== History ==\n\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment. \nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published  the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff\u2013Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\n\n\n== Etymology ==\n\nAlthough first proposed in 1956, the term \"computer science\" appears in a 1959 article in Communications of the ACM,\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921, justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM\u2014turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), inform\u00e1tica (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (\u03c0\u03bb\u03b7\u03c1\u03bf\u03c6\u03bf\u03c1\u03b9\u03ba\u03ae, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics of the University of Edinburgh).  \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"A folkloric quotation, often attributed to\u2014but almost certainly not first formulated by\u2014Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, and logic.\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt G\u00f6del, Alan Turing, John von Neumann, R\u00f3zsa P\u00e9ter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term \"Software Engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n\n\n== Philosophy ==\n\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.\n\n\n== Fields ==\nComputer science is no more about computers than astronomy is about telescopes.\n\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.CSAB, formerly called Computing Sciences Accreditation Board\u2014which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)\u2014identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human\u2013computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.\n\n\n=== Theoretical computer science ===\n\nTheoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\n\n\n==== Theory of computation ====\n\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.\n\n\n==== Information and coding theory ====\n\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\n\n\n==== Data structures and algorithms ====\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\n\n\n==== Programming language theory and formal methods ====\n\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\n\n\n=== Computer systems and computational processes ===\n\n\n==== Artificial intelligence ====\n\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\n\n\n==== Computer architecture and organization ====\n\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \u201carchitecture\u201d in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.\n\n\n==== Concurrent, parallel and distributed computing ====\n\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.\n\n\n==== Computer networks ====\n\nThis branch of computer science aims to manage networks between computers worldwide.\n\n\n==== Computer security and cryptography ====\n\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.\n\n\n==== Databases and data mining ====\n\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\n\n\n==== Computer graphics and visualization ====\n\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\n\n\n==== Image and sound processing ====\n\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.\n\n\n=== Applied computer science ===\n\n\n==== Computational science, finance and engineering ====\n\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.\n\n\n==== Social computing and human-computer interaction ====\n\nSocial computing is an area that is concerned with the intersection of social behavior and computational systems. Human-computer interaction research develops theories, principles, and guidelines for user interface designers.\n\n\n==== Software engineering ====\n\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software\u2014it doesn't just deal with the creation or manufacture of new software, but its internal arrangement and maintenance.\n\n\n== Discoveries ==\nThe philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:\nGottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent \"anything\".All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as \"on/off\", \"magnetized/de-magnetized\", \"high-voltage/low-voltage\", etc.).\nAlan Turing's insight: there are only five actions that a computer has to perform in order to do \"anything\".Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:move left one location;\nmove right one location;\nread symbol at current location;\nprint 0 at current location;\nprint 1 at current location.\nCorrado B\u00f6hm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do \"anything\".Only three rules are needed to combine any set of basic instructions into more complex ones:\nsequence: first do this, then do that;\n selection: IF such-and-such is the case, THEN do this, ELSE do that;\nrepetition: WHILE such-and-such is the case, DO this.\nNote that the three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).\n\n\n== Programming paradigms ==\n\nProgramming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\n\nFunctional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.\nImperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\nObject-oriented programming, a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.\n\n\n== Academia ==\n\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.\n\n\n== Education ==\n\nComputer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11\u201316-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nComputer science at Curlie\nScholarly Societies in Computer Science\nWhat is Computer Science?\nBest Papers Awards in Computer Science since 1996\nPhotographs of computer scientists by Bertrand Meyer\nEECS.berkeley.edu\n\n\n=== Bibliography and academic search engines ===\nCiteSeerx (article): search engine, digital library and repository for scientific and academic papers with a focus on computer and information science.\nDBLP Computer Science Bibliography (article): computer science bibliography website hosted at Universit\u00e4t Trier, in Germany.\nThe Collection of Computer Science Bibliographies (Collection of Computer Science Bibliographies)\n\n\n=== Professional organizations ===\nAssociation for Computing Machinery\nIEEE Computer Society\nInformatics Europe\nAAAI\nAAAS Computer Science\n\n\n=== Misc ===\nComputer Science\u2014Stack Exchange: a community-run question-and-answer site for computer science\nWhat is computer science\nIs computer science science?\nComputer Science (Software) Must be Considered as an Independent Discipline."
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Computer_Sciences_Corporation
Computer Sciences Corporation - Wikipedia
Computer Sciences Corporation (CSC) was an American multinational corporation that provided information technology (IT) services and professional services. On April 3, 2017, it merged with the Enterprise Services line of business of HP Enterprise (formerly Electronic Data Systems) to create DXC Technology.\n\n\n== History ==\nCSC was founded in April 1959 in Los Angeles, California, by Roy Nutt and Fletcher Jones. CSC initially provided programming tools such as assembler and compiler software.In the 1960s, CSC provided software programming services to major computer manufacturers like IBM and Honeywell and secured their first contracts for the U.S. public sector with NASA (among others).By 1963, CSC became the largest software company in the United States and the first software company to be listed on the American Stock Exchange. By the end of 1968, CSC was listed on the New York Stock Exchange and had operations in Canada, India, the United Kingdom, Germany, Spain, Italy, Brazil, and the Netherlands.\nIn 1967, CSC set up Computicket Corp. to compete in the fledgling electronic ticket market competing with Ticketron but lost $13 million and discontinued the service in 1970.In the 1970s and 1980s, CSC expanded globally winning large contracts for the finance and defense industries and through acquisitions in Europe and Australia.\nIn 2000 CSC founded a joint-venture called Innovative Banking Solutions AG in Wiesbaden, Germany to market their newly developed SAP solution for mortgage companies.\nSince its beginnings in 1959, company headquarters had been in California.  On March 29, 2008, the corporate headquarters of the company were relocated from El Segundo, California, to Annandale, Virginia.\nCSC has been a Fortune 500 Company since 1995, coming in at 162 in the 2012 rankings.In fiscal 2013, CSC acquired ServiceMesh (cloud management) for $282M, Infochimps (a big data platform) for $27M, 42Six (analytics for national intelligence) for $35 million, iSOFT (application solutions) for cash and debt, and AppLabs (application testing) for $171M.In May 2015, CSC announced plans to split the public sector business from its commercial and international business.  In August, it was announced that CSC's Government Service business would merge with SRA International to form a new company -- CSRA\u2014at the end of November 2015.In July 2015, CSC and HCL Technologies announced the signing of a joint venture agreement to form a banking software and services company, Celeriti FinTech.In September 2015, CSC closed the acquisition of Fixnetix, a provider of front-office managed trading solutions in capital markets. CSC also acquired Fruition Partners, a provider of technology-enabled solutions for the service-management sector during the month.In November 2015, CSC agreed to acquire the shares of UXC, an IT services company based in Australia in a deal valued at A$427.6 million (US$309 million).In December 2015, business technology and services provider, Xchanging, agreed to be purchased by CSC.In February 2016, CSC announced it was moving its headquarters to Tysons, Fairfax County's central business district, just a few miles from Annandale.On April 3, 2017, it merged with  HP Enterprise Services to create DXC Technology.\n\n\n== Business ==\nCSC ranked among the leading IT service providers in the world. Geographically, CSC had major operations throughout North America, Europe, Asia, and Australia.\nThe company operated in three broad service lines or sectors until the 2015 divestment of NPS, its public sector:\n\nNorth American Public Sector (NPS): Since 1961, CSC had been one of the major IT service providers for the  U.S. federal government. CSC provided services to the United States Department of Defense, law enforcement and intelligence agencies  (FBI, CIA, Homeland Security), aeronautics and aerospace agencies  (NASA). In 2012, U.S. federal contracts accounted for 36% of CSC total revenue.\nManaged Services\nBusiness Solutions and ServicesThe company  made several acquisitions, including DynCorp in 2003 and Covansys Corporation in 2007.\n\n\n== Awards and recognition ==\nIn September 2012, CSC was ranked 8th in Software Magazine\u2019s Software 500 ranking of the world\u2019s largest software and service providers.\n\n\n== Criticism ==\nIn June 2013, Margaret Hodge, chair of the Public Accounts Committee, a Select committee of the British House of Commons, described CSC as a \"rotten company providing a hopeless system\" with reference to their multibillion-pound contract to deliver the National Programme for IT Lorenzo contract.\nIn December 2011, the non-partisan organization Public Campaign criticized CSC for spending US$4.39 million on lobbying and not paying any taxes during 2008\u201310, instead getting US$305 million in tax rebates, despite making a profit of US$1.67 billion.\nIn February 2011, the U.S. Securities and Exchange Commission (SEC) launched a fraud investigation into CSC's accounting practices in Denmark and Australian business. CSC's CFO Mike Mancuso confirmed that accounting errors and intentional misconduct by certain personnel in Australia prompted SEC regulators to turn their gaze to Australia. Mancuso also stated that the alleged misconduct includes US$19 million in both intentional accounting irregularities and unintentional accounting errors. The SEC accused the former CEO Mike LAPHEN of fraud and clawed back $4,35 million\nThe company has been accused of breaching human rights by arranging several illegal rendition flights for the CIA between 2003 and 2006, which also has led to criticism of shareholders of the company, including the governments of Norway and Britain.\nThe company has engaged in a number of activities that have resulted in legal action against it. These are:\nIts so called WorldBridge Service (Visa Services), which processed and issued millions of visa applications to enter Britain, did not involve British authorities.\nCSC was one of the contractors hired by the Internal Revenue Service to modernize its tax-filing system. They told the IRS it would meet a January 2006 deadline, but failed to do so, leaving the IRS with no system capable of detecting fraud. Its failure to meet the delivery deadline for developing an automated refund fraud detection system cost the IRS between US$200 million and US$300 million.\n\n\n== See also ==\nVP/MS (Visual Product Modeling System), a modeling language and product lifecycle management tool by CSC\nTop 100 US Federal Contractors\nCSRA Inc., formed by a merger of CSC's North American Public Sector and SRA International\nTeam CSC and other names 2003-2010. Professional cycling sponsorship under team owner Bjarne Riis.\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nComputer Sciences Corporation SEC Filings"
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Programming_language_theory
Programming language theory - Wikipedia
Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and of their individual features.  It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science.  It has become a well-recognized branch of computer science, and an active research area, with results published in numerous  journals dedicated to PLT, as well as in general computer science and engineering publications.\n\n\n== History ==\nIn some ways, the history of programming language theory predates even the development of programming languages themselves.  The lambda calculus, developed by Alonzo Church and Stephen Cole Kleene in the 1930s, is considered by some to be the world's first programming language, even though it was intended to model computation rather than being a means for programmers to describe algorithms to a computer system.  Many modern functional programming languages have been described as providing a \"thin veneer\" over the lambda calculus, and many are easily described in terms of it.\nThe first programming language to be invented was Plankalk\u00fcl, which was designed by Konrad Zuse in the 1940s, but not publicly known until 1972 (and not implemented until 1998).  The first widely known and successful high-level programming language was Fortran, developed from 1954 to 1957 by a team of IBM researchers led by John Backus.  The success of FORTRAN led to the formation of a committee of scientists to develop a \"universal\" computer language; the result of their effort was ALGOL 58.  Separately, John McCarthy of MIT developed the Lisp programming language (based on the lambda calculus), the first language with origins in academia to be successful.  With the success of these initial efforts, programming languages became an active topic of research in the 1960s and beyond.\nSome other key events in the history of programming language theory since then:\n\n\n=== 1950s ===\nNoam Chomsky developed the Chomsky hierarchy in the field of linguistics; a discovery which has directly impacted programming language theory and other branches of computer science.\n\n\n=== 1960s ===\nThe Simula language was developed by Ole-Johan Dahl and Kristen Nygaard; it is widely considered to be the first example of an object-oriented programming language; Simula also introduced the concept of coroutines.\nIn 1964, Peter Landin is the first to realize Church's lambda calculus can be used to model programming languages. He introduces the SECD machine which \"interprets\" lambda expressions.\nIn 1965, Landin introduces the J operator, essentially a form of continuation.\nIn 1966, Landin introduces ISWIM, an abstract computer programming language in his article The Next 700 Programming Languages. It is influential in the design of languages leading to the Haskell programming language.\nIn 1966, Corrado B\u00f6hm introduced the programming language CUCH (Curry-Church).\nIn 1967, Christopher Strachey publishes his influential set of lecture notes Fundamental Concepts in Programming Languages, introducing the terminology R-values, L-values, parametric polymorphism, and ad hoc polymorphism.\nIn 1969, J. Roger Hindley publishes The Principal Type-Scheme of an Object in Combinatory Logic, later generalized into the Hindley\u2013Milner type inference algorithm.\nIn 1969, Tony Hoare introduces the Hoare logic, a form of axiomatic semantics.\nIn 1969, William Alvin Howard observed that a \"high-level\" proof system, referred to as natural deduction, can be directly interpreted in its intuitionistic version as a typed variant of the model of computation known as lambda calculus. This became known as the Curry\u2013Howard correspondence.\n\n\n=== 1970s ===\nIn 1970, Dana Scott first publishes his work on denotational semantics.\nIn 1972, logic programming and Prolog were developed thus allowing computer programs to be expressed as mathematical logic.\nA team of scientists at Xerox PARC led by Alan Kay develop Smalltalk, an object-oriented language widely known for its innovative development environment.\nIn 1974, John C. Reynolds discovers System F. It had already been discovered in 1971 by the mathematical logician Jean-Yves Girard.\nFrom 1975, Gerald Jay Sussman and Guy Steele develop the Scheme programming language, a Lisp dialect incorporating lexical scoping, a unified namespace, and elements from the actor model including first-class continuations.\nBackus, at the 1977 ACM Turing Award lecture, assailed the current state of industrial languages and proposed a new class of programming languages now known as function-level programming languages.\nIn 1977, Gordon Plotkin introduces Programming Computable Functions, an abstract typed functional language.\nIn 1978, Robin Milner introduces the Hindley\u2013Milner type inference algorithm for the ML programming language. Type theory became applied as a discipline to programming languages, this application has led to tremendous advances in type theory over the years.\n\n\n=== 1980s ===\nIn 1981, Gordon Plotkin publishes his paper on structured operational semantics.\nIn 1988, Gilles Kahn published his paper on natural semantics.\nThere emerged process calculi, such as the Calculus of Communicating Systems of Robin Milner, and the Communicating sequential processes model of C. A. R. Hoare, as well as similar models of concurrency such as the actor model of Carl Hewitt.\nIn 1985, the release of Miranda sparks an academic interest in lazy-evaluated pure functional programming languages. A committee was formed to define an open standard resulting in the release of the Haskell 1.0 standard in 1990.\nBertrand Meyer created the methodology Design by contract and incorporated it into the Eiffel programming language.\n\n\n=== 1990s ===\nGregor Kiczales, Jim Des Rivieres and Daniel G. Bobrow published the book The Art of the Metaobject Protocol.\nEugenio Moggi and Philip Wadler introduced the use of monads for structuring programs written in functional programming languages.\n\n\n== Sub-disciplines and related fields ==\nThere are several fields of study which either lie within programming language theory, or which have a profound influence on it; many of these have considerable overlap. In addition, PLT makes use of many other branches of mathematics, including computability theory, category theory, and set theory.\n\n\n=== Formal semantics ===\n\nFormal semantics is the formal specification of the behaviour of computer programs and programming languages. Three common approaches to describe the semantics or \"meaning\" of a computer program are denotational semantics, operational semantics and axiomatic semantics.\n\n\n=== Type theory ===\n\nType theory is the study of type systems; which are \"a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute\". Many programming languages are distinguished by the characteristics of their type systems.\n\n\n=== Program analysis and transformation ===\n\nProgram analysis is the general problem of examining a program and determining key characteristics (such as the absence of classes of program errors). Program transformation is the process of transforming a program in one form (language) to another form.\n\n\n=== Comparative programming language analysis ===\nComparative programming language analysis seeks to classify programming languages into different types based on their characteristics; broad categories of programming languages are often known as programming paradigms.\n\n\n=== Generic and metaprogramming ===\nMetaprogramming is the generation of higher-order programs which, when executed, produce programs (possibly in a different language, or in a subset of the original language) as a result.\n\n\n=== Domain-specific languages ===\nDomain-specific languages are languages constructed to efficiently solve problems of a particular part of domain.\n\n\n=== Compiler construction ===\n\nCompiler theory is the theory of writing compilers (or more generally, translators); programs which translate a program written in one language into another form.  The actions of a compiler are traditionally broken up into syntax analysis (scanning and parsing), semantic analysis (determining what a program should do), optimization (improving the performance of a program as indicated by some metric; typically execution speed) and code generation (generation and output of an equivalent program in some target language; often the instruction set of a CPU).\n\n\n=== Run-time systems ===\nRuntime systems refers to the development of programming language runtime environments and their components, including virtual machines, garbage collection, and foreign function interfaces.\n\n\n== Journals, publications, and conferences ==\nConferences are the primary venue for presenting research in programming languages. The most well known conferences include the Symposium on Principles of Programming Languages (POPL), Programming Language Design and Implementation (PLDI), the International Conference on Functional Programming (ICFP), the International Conference on Object Oriented Programming, Systems, Languages and Applications (OOPSLA) and the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). \nNotable journals that publish PLT research include the ACM Transactions on Programming Languages and Systems (TOPLAS), Journal of Functional Programming (JFP), Journal of Functional and Logic Programming, and Higher-Order and Symbolic Computation.\n\n\n== See also ==\nSIGPLAN\nTimeline of programming languages\nVery high-level programming language\n\n\n== References ==\n\n\n== Further reading ==\n\nAbadi, Mart\u00edn and Cardelli, Luca. A Theory of Objects. Springer-Verlag.\nMichael J. C. Gordon. Programming Language Theory and Its Implementation. Prentice Hall.\nGunter, Carl and Mitchell, John C. (eds.). Theoretical Aspects of Object Oriented Programming Languages: Types, Semantics, and Language Design. MIT Press.\nHarper, Robert. Practical Foundations for Programming Languages. Draft version.\nKnuth, Donald E. (2003). Selected Papers on Computer Languages. Stanford, California: Center for the Study of Language and Information.\nMitchell, John C.. Foundations for Programming Languages.\nMitchell, John C.. Introduction to Programming Language Theory.\nO'Hearn, Peter. W. and Tennent, Robert. D. (1997). Algol-like Languages. Progress in Theoretical Computer Science. Birkhauser, Boston.\nPierce, Benjamin C. (2002). Types and Programming Languages. MIT Press.\nPierce, Benjamin C. Advanced Topics in Types and Programming Languages.\nPierce, Benjamin C. et al. (2010). Software Foundations.\n\n\n== External links ==\nLambda the Ultimate, a community weblog for professional discussion and repository of documents on programming language theory.\nGreat Works in Programming Languages. Collected by Benjamin C. Pierce (University of Pennsylvania).\nClassic Papers in Programming Languages and Logic. Collected by Karl Crary (Carnegie Mellon University).\nProgramming Language Research. Directory by Mark Leone.\nProgramming Language Theory Texts Online. At Utrecht University.\n\u03bb-Calculus: Then & Now by Dana S. Scott for the ACM Turing Centenary Celebration\nGrand Challenges in Programming Languages. Panel session at POPL 2009."
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Computational_complexity_theory
Computational complexity theory - Wikipedia
Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\nA problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.\n\n\n== Computational problems ==\n\n\n=== Problem instances ===\nA computational problem can be viewed as an infinite collection of instances together with a solution for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g., 15) and the solution is \"yes\" if the number is prime and \"no\" otherwise (in this case, 15 is not prime and the answer is \"no\"). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.\nTo further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany's 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.\n\n\n=== Representing problem instances ===\nWhen considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.\nEven though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.\n\n\n=== Decision problems as formal languages ===\n\nDecision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either yes or no, or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer yes, the algorithm is said to accept the input string, otherwise it is said to reject the input.\nAn example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected or not. The formal language associated with this decision problem is then the set of all connected graphs \u2014 to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.\n\n\n=== Function problems ===\nA function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem\u2014that is, the output isn't just yes or no. Notable examples include the traveling salesman problem and the integer factorization problem.\nIt is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples (a, b, c) such that the relation a \u00d7 b = c holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.\n\n\n=== Measuring the size of an instance ===\nTo measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2n vertices compared to the time taken for a graph with n vertices?\nIf the input size is n, the time taken can be expressed as a function of n. Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n. If T(n) is a polynomial in n, then the algorithm is said to be a polynomial time algorithm. Cobham's thesis argues that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.\n\n\n== Machine models and complexity measures ==\n\n\n=== Turing machine ===\n\nA Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a general model of a computing machine\u2014anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church\u2013Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.\nMany types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.\nA deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.\n\n\n=== Other machine models ===\nMany machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary. What all these models have in common is that the machines operate deterministically.\nHowever, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.\n\n\n=== Complexity measures ===\nFor a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer (\"yes\" or \"no\"). A Turing machine M is said to operate within time f(n) if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).\nAnalogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.\nThe complexity of an algorithm is often expressed using big O notation.\n\n\n=== Best, worst and average case complexity ===\n\nThe best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size n may be faster to solve than others, we define the following complexities:\n\nBest-case complexity: This is the complexity of solving the problem for the best input of size n.\nAverage-case complexity: This is the complexity of solving the problem on an average. This complexity is only defined with respect to a probability distribution over the inputs. For instance, if all inputs of the same size are assumed to be equally likely to appear, the average case complexity can be defined with respect to the uniform distribution over all inputs of size n.\nAmortized analysis: Amortized analysis considers both the costly and less costly operations together over the whole series of operations of the algorithm.\nWorst-case complexity: This is the complexity of solving the problem for the worst input of size n.The order from cheap to costly is: Best, average (of discrete uniform distribution), amortized, worst.\nFor example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the pivot is always the largest or smallest value in the list (so the list is never divided). In this case the algorithm takes time O(n2). If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.\n\n\n=== Upper and lower bounds on the complexity of problems ===\nTo classify the computation time (or similar resources, such as space consumption), it is helpful to demonstrate upper and lower bounds on the maximum amount of time required by the most efficient algorithm to solve a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most T(n). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase \"all possible algorithms\" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of T(n) for a problem requires showing that no algorithm can have time complexity lower than T(n).\nUpper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if T(n) = 7n2 + 15n + 40, in big O notation one would write T(n) = O(n2).\n\n\n== Complexity classes ==\n\n\n=== Defining complexity classes ===\nA complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:\n\nThe type of computational problem: The most commonly used problems are decision problems. However, complexity classes can be defined based on function problems, counting problems, optimization problems, promise problems, etc.\nThe model of computation: The most common model of computation is the deterministic Turing machine, but many complexity classes are based on non-deterministic Turing machines, Boolean circuits, quantum Turing machines, monotone circuits, etc.\nThe resource (or resources) that is being bounded and the bound: These two properties are usually stated together, such as \"polynomial time\", \"logarithmic space\", \"constant depth\", etc.Some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:\n\nThe set of decision problems solvable by a deterministic Turing machine within time f(n). (This complexity class is known as DTIME(f(n)).)But bounding the computation time above by some concrete function f(n) often yields complexity classes that depend on the chosen machine model. For instance, the language {xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that \"the time complexities in any two reasonable and general models of computation are polynomially related\" (Goldreich 2008, Chapter 1.2). This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.\n\n\n=== Important complexity classes ===\n\nMany important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:\n\nThe logarithmic-space classes (necessarily) do not take into account the space needed to represent the problem.\nIt turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch's theorem.\nOther important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.\n\n\n=== Hierarchy theorems ===\n\nFor the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(n) is contained in DTIME(n2), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.\nMore precisely, the time hierarchy theorem states that\n\n  \n    \n      \n        \n          \n            D\n            T\n            I\n            M\n            E\n          \n        \n        \n          \n            (\n          \n        \n        f\n        (\n        n\n        )\n        \n          \n            )\n          \n        \n        \u228a\n        \n          \n            D\n            T\n            I\n            M\n            E\n          \n        \n        \n          \n            (\n          \n        \n        f\n        (\n        n\n        )\n        \u22c5\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        (\n        f\n        (\n        n\n        )\n        )\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {DTIME}}{\\big (}f(n){\\big )}\\subsetneq {\\mathsf {DTIME}}{\\big (}f(n)\\cdot \\log ^{2}(f(n)){\\big )}}\n  .The space hierarchy theorem states that\n\n  \n    \n      \n        \n          \n            D\n            S\n            P\n            A\n            C\n            E\n          \n        \n        \n          \n            (\n          \n        \n        f\n        (\n        n\n        )\n        \n          \n            )\n          \n        \n        \u228a\n        \n          \n            D\n            S\n            P\n            A\n            C\n            E\n          \n        \n        \n          \n            (\n          \n        \n        f\n        (\n        n\n        )\n        \u22c5\n        log\n        \u2061\n        (\n        f\n        (\n        n\n        )\n        )\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {DSPACE}}{\\big (}f(n){\\big )}\\subsetneq {\\mathsf {DSPACE}}{\\big (}f(n)\\cdot \\log(f(n)){\\big )}}\n  .The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.\n\n\n=== Reduction ===\n\nMany complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at most as difficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is no more difficult than Y, and we say that X reduces to Y. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.\nThe most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.\nThis motivates the concept of a problem being hard for a complexity class. A problem X is hard for a class of problems C if every problem in C can be reduced to X. Thus no problem in C is harder than X, since an algorithm for X allows us to solve any problem in C. The notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.\nIf a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the hardest problem in C. (Since many problems could be equally hard, one might say that X is one of the hardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, \u03a02, to another problem, \u03a01, would indicate that there is no known polynomial-time solution for \u03a01. This is because a polynomial-time solution to \u03a01 would yield a polynomial-time solution to \u03a02. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.\n\n\n== Important open problems ==\n\n\n=== P versus NP problem ===\n\nThe complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham\u2013Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.\nThe question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.\n\n\n=== Problems in NP not known to be in P or NP-complete ===\nIt was shown by Ladner that if P \u2260 NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.\nThe graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to L\u00e1szl\u00f3 Babai and Eugene Luks has run time \n  \n    \n      \n        O\n        (\n        \n          2\n          \n            \n              n\n              log\n              \u2061\n              n\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle O(2^{\\sqrt {n\\log n}})}\n   for graphs with n vertices, although some recent work by Babai offers some potentially new perspectives on this.The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a prime factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time \n  \n    \n      \n        O\n        (\n        \n          e\n          \n            \n              (\n              \n                \n                  \n                    64\n                    9\n                  \n                  \n                    3\n                  \n                \n              \n              )\n            \n            \n              \n                \n                  (\n                  log\n                  \u2061\n                  n\n                  )\n                \n                \n                  3\n                \n              \n            \n            \n              \n                \n                  (\n                  log\n                  \u2061\n                  log\n                  \u2061\n                  n\n                  \n                    )\n                    \n                      2\n                    \n                  \n                \n                \n                  3\n                \n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle O(e^{\\left({\\sqrt[{3}]{\\frac {64}{9}}}\\right){\\sqrt[{3}]{(\\log n)}}{\\sqrt[{3}]{(\\log \\log n)^{2}}}})}\n   to factor an odd integer n. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes.\n\n\n=== Separations between other complexity classes ===\nMany known complexity classes are suspected to be unequal, but this has not been proved. For instance P \u2286 NP \u2286 PP \u2286 PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.\nAlong the same lines, co-NP is the class containing the complement problems (i.e. problems with the yes/no answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then P is not equal to NP, since P=co-P.  Thus if P=NP we would have co-P=co-NP whence NP=P=co-P=co-NP. \nSimilarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.\nIt is suspected that P and BPP are equal. However, it is currently open if BPP = NEXP.\n\n\n== Intractability ==\n\nA problem that can be solved in theory (e.g. given large but finite resources, especially time), but for which in practice any solution takes too many resources to be useful, is known as an intractable problem. Conversely, a problem that can be solved in practice is called a tractable problem, literally \"a problem that can be handled\". The term infeasible (literally \"cannot be done\") is sometimes used interchangeably with intractable, though this risks confusion with a feasible solution in mathematical optimization.Tractable problems are frequently identified with problems that have polynomial-time solutions (P, PTIME); this is known as the Cobham\u2013Edmonds thesis. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then NP-hard problems are also intractable in this sense.\nHowever, this identification is inexact: a polynomial-time solution with large degree or large leading coefficient grows quickly, and may be impractical for practical size problems; conversely, an exponential-time solution that grows slowly may be practical on realistic input, or a solution that takes a long time in the worst case may take a short time in most cases or the average case, and thus still be practical. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.\nTo see why exponential-time algorithms are generally unusable in practice, consider a program that makes 2n operations before halting. For small n, say 100, and assuming for the sake of example that the computer does 1012 operations each second, the program would run for about 4 \u00d7 1010 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. However, an exponential-time algorithm that takes 1.0001n operations is practical until n gets relatively large.\nSimilarly, a polynomial time algorithm is not always practical. If its running time is, say, n15, it is unreasonable to consider it efficient and it is still useless except on small instances. Indeed, in practice even n3 or n2 algorithms are often impractical on realistic sizes of problems.\n\n\n== Continuous complexity theory ==\nContinuous complexity theory can refer to complexity theory of problems that involve continuous functions that are approximated by discretizations, as studied in numerical analysis. One approach to complexity theory of numerical analysis is information based complexity.\nContinuous complexity theory can also refer to complexity theory of the use of analog computation, which uses continuous dynamical systems and differential equations. Control theory can be considered a form of computation and differential equations are used in the modelling of continuous-time and hybrid discrete-continuous-time systems.\n\n\n== History ==\nAn early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lam\u00e9 in 1844.\nBefore the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.\nThe beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper \"On the Computational Complexity of Algorithms\" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a \"good\" algorithm to be one with running time bounded by a polynomial of the input size.Earlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers:\n\nHowever, [my] initial interest [in automata theory] was increasingly set aside in favor of computational complexity, an exciting fusion of combinatorial methods, inherited from switching theory, with the conceptual arsenal of the theory of algorithms. These ideas had occurred to me earlier in 1955 when I coined the term \"signalizing function\", which is nowadays commonly known as \"complexity measure\".\nIn 1967, Manuel Blum formulated a set of axioms (now known as Blum axioms) specifying desirable properties of complexity measures on the set of computable functions and proved an important result, the so-called speed-up theorem. The field began to flourish in 1971 when the Stephen Cook and Leonid Levin proved the existence of practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, \"Reducibility Among Combinatorial Problems\", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.In the 1980s, much work was done on the average difficulty of solving NP-complete problems\u2014both exactly and approximately. At that time, computational complexity theory was at its height, and it was widely believed that if a problem turned out to be NP-complete, then there was little chance of being able to work with the problem in a practical situation. However, it became increasingly clear that this is not always the case, and some authors claimed that general asymptotic results are often unimportant for typical problems arising in practice.\n\n\n== See also ==\n\n\n== Works on complexity ==\nWuppuluri, Shyam; Doria, Francisco A., eds. (2020), Unravelling Complexity: The Life and Work of Gregory Chaitin, World Scientific, doi:10.1142/11270, ISBN 978-981-12-0006-9\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Textbooks ===\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity: A Modern Approach, Cambridge University Press, ISBN 978-0-521-42426-4, Zbl 1193.68112\nDowney, Rod; Fellows, Michael (1999), Parameterized complexity, Monographs in Computer Science, Berlin, New York: Springer-Verlag, ISBN 9780387948836\nDu, Ding-Zhu; Ko, Ker-I (2000), Theory of Computational Complexity, John Wiley & Sons, ISBN 978-0-471-34506-0\nGarey, Michael R.; Johnson, David S. (1979), Computers and Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman, ISBN 0-7167-1045-5\nGoldreich, Oded (2008), Computational Complexity: A Conceptual Perspective, Cambridge University Press\nvan Leeuwen, Jan, ed. (1990), Handbook of theoretical computer science (vol. A): algorithms and complexity, MIT Press, ISBN 978-0-444-88071-0\nPapadimitriou, Christos (1994), Computational Complexity (1st ed.), Addison Wesley, ISBN 978-0-201-53082-7\nSipser, Michael (2006), Introduction to the Theory of Computation (2nd ed.), USA: Thomson Course Technology, ISBN 978-0-534-95097-2\n\n\n=== Surveys ===\nKhalil, Hatem; Ulery, Dana (1976), \"A Review of Current Studies on Complexity of Algorithms for Partial Differential Equations\", Proceedings of the Annual Conference on - ACM 76, ACM '76: 197\u2013201, doi:10.1145/800191.805573, S2CID 15497394\nCook, Stephen (1983), \"An overview of computational complexity\" (PDF), Commun. ACM, 26 (6): 400\u2013408, doi:10.1145/358141.358144, ISSN 0001-0782, S2CID 14323396, archived from the original (PDF) on July 22, 2018, retrieved October 24, 2017\nFortnow, Lance; Homer, Steven (2003), \"A Short History of Computational Complexity\" (PDF), Bulletin of the EATCS, 80: 95\u2013133\nMertens, Stephan (2002), \"Computational Complexity for Physicists\", Computing in Science and Eng., 4 (3): 31\u201347, arXiv:cond-mat/0012185, Bibcode:2002CSE.....4c..31M, doi:10.1109/5992.998639, ISSN 1521-9615, S2CID 633346\n\n\n== External links ==\nThe Complexity Zoo\n\"Computational complexity classes\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nWhat are the most important results (and papers) in complexity theory that every one should know?\nScott Aaronson: Why Philosophers Should Care About Computational Complexity"
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Artificial_intelligence
Artificial intelligence - Wikipedia
Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality. The distinction between the former and the latter categories is often revealed by the acronym chosen. 'Strong' AI is usually labelled as AGI (Artificial General Intelligence) while attempts to emulate 'natural' intelligence have been called ABI (Artificial Biological Intelligence). Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\".As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology. Modern machine capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations.Artificial intelligence was founded as an academic discipline in 1955, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. After AlphaGo successfully defeated a professional Go player in 2015, artificial intelligence once again attracted widespread global attention. For most of its history, AI research has been divided into sub-fields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. \"robotics\" or \"machine learning\"), the use of particular tools (\"logic\" or artificial neural networks), or deep philosophical differences. Sub-fields have also been based on social factors (particular institutions or the work of particular researchers).The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The AI field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields.\nThe field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\". This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI to be a danger to humanity if it progresses unabated. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.\n\n\n== History ==\n\nThought-capable artificial beings appeared as storytelling devices in antiquity, and have been common in fiction, as in Mary Shelley's Frankenstein or Karel \u010capek's R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church\u2013Turing thesis. Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. Turing proposed changing the question from whether a machine was intelligent, to \"whether or not it is possible for machinery to show intelligent behaviour\". The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".The field of AI research was born at a workshop at Dartmouth College in 1956, where the term \"Artificial Intelligence\" was coined by John McCarthy to distinguish the field from cybernetics and escape the influence of the cyberneticist Norbert Wiener. Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies (c. 1954) (and by 1959 were reportedly playing better than the average human), solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.The development of metal\u2013oxide\u2013semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) transistor technology, enabled the development of practical artificial neural network (ANN) technology in the 1980s. A landmark publication in the field was the 1989 book Analog VLSI Implementation of Neural Systems by Carver A. Mead and Mohammed Ismail.In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas. The success was due to increasing computational power (see Moore's law and transistor count), greater emphasis on solving specific problems, new ties between AI and other fields (such as statistics, economics and mathematics), and a commitment by researchers to mathematical methods and scientific standards. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.In 2011, a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012. The Kinect, which provides a 3D body\u2013motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is a relatively complex game, more so than Chess.\nAccording to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. Clark also presents factual data indicating the improvements of AI since 2012 supported by lower error rates in image processing tasks. He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people. In a 2017 survey, one in five companies reported they had \"incorporated AI in some offerings or processes\". Around 2016, China greatly accelerated its government funding; given its large supply of data and its rapidly increasing research output, some observers believe it may be on track to becoming an \"AI superpower\". However, it has been acknowledged that reports regarding artificial intelligence have tended to be exaggerated.\n\n\n== Basics ==\nComputer science defines AI research as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. A more elaborate definition characterizes AI as \"a system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.\"A typical AI analyzes its environment and takes actions that maximize its chance of success. An AI's intended utility function (or goal) can be simple (\"1 if the AI wins a game of Go, 0 otherwise\") or complex (\"Perform actions mathematically similar to ones that succeeded in the past\"). Goals can be explicitly defined or induced. If the AI is programmed for \"reinforcement learning\", goals can be implicitly induced by rewarding some types of behavior or punishing others. Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food. Some AI systems, such as nearest-neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data. Such systems can still be benchmarked if the non-goal system is framed as a system whose \"goal\" is to successfully accomplish its narrow classification task.AI often revolves around the use of algorithms. An algorithm is a set of unambiguous instructions that a mechanical computer can execute. A complex algorithm is often built on top of other, simpler, algorithms. A simple example of an algorithm is the following (optimal for first player) recipe for play at tic-tac-toe:\nIf someone has a \"threat\" (that is, two in a row), take the remaining square. Otherwise,\nif a move \"forks\" to create two threats at once, play that move. Otherwise,\ntake the center square if it is free. Otherwise,\nif your opponent has played in a corner, take the opposite corner. Otherwise,\ntake an empty corner if one exists. Otherwise,\ntake any empty square.Many AI algorithms are capable of learning from data; they can enhance themselves by learning new heuristics (strategies, or \"rules of thumb\", that have worked well in the past), or can themselves write other algorithms. Some of the \"learners\" described below, including Bayesian networks, decision trees, and nearest-neighbor, could theoretically, (given infinite data, time, and memory) learn to approximate any function, including which combination of mathematical functions would best describe the world. These learners could therefore derive all possible knowledge, by considering every possible hypothesis and matching them against the data. In practice, it is seldom possible to consider every possibility, because of the phenomenon of \"combinatorial explosion\", where the time needed to solve a problem grows exponentially. Much of AI research involves figuring out how to identify and avoid considering a broad range of possibilities unlikely to be beneficial. For example, when viewing a map and looking for the shortest driving route from Denver to New York in the East, one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus, an AI wielding a pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered.The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): \"If an otherwise healthy adult has a fever, then they may have influenza\". A second, more general, approach is Bayesian inference: \"If the current patient has a fever, adjust the probability they have influenza in such-and-such way\". The third major approach, extremely popular in routine business AI applications, are analogizers such as SVM and nearest-neighbor: \"After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza\". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial \"neurons\" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to \"reinforce\" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem.Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". They can be nuanced, such as \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\". Learners also work on the basis of \"Occam's razor\": The simplest theory that explains the data is the likeliest. Therefore, according to Occam's razor principle, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better.\n\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is. Besides classic overfitting, learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often don't primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.\n\nCompared with humans, existing AI lacks several features of human \"commonsense reasoning\"; most notably, humans have powerful mechanisms for reasoning about \"na\u00efve physics\" such as space, time, and physical interactions. This enables even young children to easily make inferences like \"If I roll this pen off a table, it will fall on the floor\". Humans also have a powerful mechanism of \"folk psychology\" that helps them to interpret natural-language sentences such as \"The city councilmen refused the demonstrators a permit because they advocated violence\" (A generic AI has difficulty discerning whether the ones alleged to be advocating violence are the councilmen or the demonstrators). This lack of \"common knowledge\" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents.\n\n\n== Challenges ==\nThe cognitive capabilities of current architectures are very limited, using only a simplified version of what intelligence is really capable of. For instance, the human mind has come up with ways to reason beyond measure and logical explanations to different occurrences in life. What would have been otherwise straightforward, an equivalently difficult problem may be challenging to solve computationally as opposed to using the human mind. This gives rise to two classes of models: structuralist and functionalist. The structural models aim to loosely mimic the basic intelligence operations of the mind such as reasoning and logic. The functional model refers to the correlating data to its computed counterpart.The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.\n\n\n=== Reasoning, problem solving ===\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.These algorithms proved to be insufficient for solving large reasoning problems because they experienced a \"combinatorial explosion\": they became exponentially slower as the problems grew larger. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.\n\n\n=== Knowledge representation ===\n\nKnowledge representation and knowledge engineering are central to classical AI research. Some \"expert systems\" attempt to gather explicit knowledge possessed by experts in some narrow domain. In addition, some projects attempt to gather the \"commonsense knowledge\" known to the average person into a database containing extensive knowledge about the world. Among the things a comprehensive commonsense knowledge base would contain are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations can be used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.Among the most difficult problems in knowledge representation are:\n\nDefault reasoning and the qualification problem\nMany of the things people know take the form of \"working assumptions\". For example, if a bird comes up in conversation, people typically picture a fist-sized animal that sings and flies. None of these things are true about all birds. John McCarthy identified this problem in 1969 as the qualification problem: for any commonsense rule that AI researchers care to represent, there tend to be a huge number of exceptions. Almost nothing is simply true or false in the way that abstract logic requires. AI research has explored a number of solutions to this problem.\nBreadth of commonsense knowledge\nThe number of atomic facts that the average person knows is very large. Research projects that attempt to build a complete knowledge base of commonsense knowledge (e.g., Cyc) require enormous amounts of laborious ontological engineering\u2014they must be built, by hand, one complicated concept at a time.\nSubsymbolic form of some commonsense knowledge\nMuch of what people know is not represented as \"facts\" or \"statements\" that they could express verbally. For example, a chess master will avoid a particular chess position because it \"feels too exposed\" or an art critic can take one look at a statue and realize that it is a fake. These are non-conscious and sub-symbolic intuitions or tendencies in the human brain. Knowledge like this informs, supports and provides a context for symbolic, conscious knowledge. As with the related problem of sub-symbolic reasoning, it is hoped that situated AI, computational intelligence, or statistical AI will provide ways to represent this knowledge.\n\n\n=== Planning ===\n\nIntelligent agents must be able to set goals and achieve them. They need a way to visualize the future\u2014a representation of the state of the world and be able to make predictions about how their actions will change it\u2014and be able to make choices that maximize the utility (or \"value\") of available choices.In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions. However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions but also evaluate its predictions and adapt based on its assessment.Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.\n\n\n=== Learning ===\n\nMachine learning (ML), a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.Unsupervised learning is the ability to find patterns in a stream of input, without requiring a human to label the inputs first. Supervised learning includes both classification and numerical regression, which requires a human to label the input data first. Classification is used to determine what category something belongs in, and occurs after a program sees a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as \"function approximators\" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, \"spam\" or \"not spam\". Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space.\n\n\n=== Natural language processing ===\n\nNatural language processing (NLP) allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering and machine translation. Many current approaches use word co-occurrence frequencies to construct syntactic representations of text. \"Keyword spotting\" strategies for search are popular and scalable but dumb; a search query for \"dog\" might only match documents with the literal word \"dog\" and miss a document with the word \"poodle\". \"Lexical affinity\" strategies use the occurrence of words such as \"accident\" to assess the sentiment of a document. Modern statistical NLP approaches can combine all these strategies as well as others, and often achieve acceptable accuracy at the page or paragraph level. Beyond semantic NLP, the ultimate goal of \"narrative\" NLP is to embody a full understanding of commonsense reasoning. By 2019, transformer-based deep learning architectures could generate coherent text.\n\n\n=== Perception ===\n\nMachine perception is the ability to use input from sensors (such as cameras (visible spectrum or infrared), microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition, facial recognition, and object recognition. Computer vision is the ability to analyze visual input. Such input is usually ambiguous; a giant, fifty-meter-tall pedestrian far away may produce the same pixels as a nearby normal-sized pedestrian, requiring the AI to judge the relative likelihood and reasonableness of different interpretations, for example by using its \"object model\" to assess that fifty-meter pedestrians do not exist.\n\n\n=== Motion and manipulation ===\n\nAI is heavily used in robotics. Advanced robotic arms and other industrial robots, widely used in modern factories, can learn from experience how to move efficiently despite the presence of friction and gear slippage. A modern mobile robot, when given a small, static, and visible environment, can easily determine its location and map its environment; however, dynamic environments, such as (in endoscopy) the interior of a patient's breathing body, pose a greater challenge. Motion planning is the process of breaking down a movement task into \"primitives\" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. Moravec's paradox generalizes that low-level sensorimotor skills that humans take for granted are, counterintuitively, difficult to program into a robot; the paradox is named after Hans Moravec, who stated in 1988 that \"it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility\". This is attributed to the fact that, unlike checkers, physical dexterity has been a direct target of natural selection for millions of years.\n\n\n=== Social intelligence ===\n\nMoravec's paradox can be extended to many forms of social intelligence. Distributed multi-agent coordination of autonomous vehicles remains a difficult problem. Affective computing is an interdisciplinary umbrella that comprises systems which recognize, interpret, process, or simulate human affects. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal affect analysis (see multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.In the long run, social skills and an understanding of human emotion and game theory would be valuable to a social agent. The ability to predict the actions of others by understanding their motives and emotional states would allow an agent to make better decisions. Some computer systems mimic human emotion and expressions to appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction. Similarly, some virtual assistants are programmed to speak conversationally or even to banter humorously; this tends to give na\u00efve users an unrealistic conception of how intelligent existing computer agents actually are.\n\n\n=== General intelligence ===\n\nHistorically, projects such as the Cyc knowledge base (1984\u2013) and the massive Japanese Fifth Generation Computer Systems initiative (1982\u20131992) attempted to cover the breadth of human cognition. These early projects failed to escape the limitations of non-quantitative symbolic logic models and, in retrospect, greatly underestimated the difficulty of cross-domain AI. Nowadays, most current AI researchers work instead on tractable \"narrow AI\" applications (such as medical diagnosis or automobile navigation). Many researchers predict that such \"narrow AI\" work in different individual domains will eventually be incorporated into a machine with artificial general intelligence (AGI), combining most of the narrow skills mentioned in this article and at some point even exceeding human ability in most or all these areas. Many advances have general, cross-domain significance. One high-profile example is that DeepMind in the 2010s developed a \"generalized artificial intelligence\" that could learn many diverse Atari games on its own, and later developed a variant of the system which succeeds at sequential learning. Besides transfer learning, hypothetical AGI breakthroughs could include the development of reflective architectures that can engage in decision-theoretic metareasoning, and figuring out how to \"slurp up\" a comprehensive knowledge base from the entire unstructured Web. Some argue that some kind of (currently-undiscovered) conceptually straightforward, but mathematically difficult, \"Master Algorithm\" could lead to AGI. Finally, a few \"emergent\" approaches look to simulating human intelligence extremely closely, and believe that anthropomorphic features like an artificial brain or simulated child development may someday reach a critical point where general intelligence emerges.Many of the problems in this article may also require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered \"AI-complete\", because all of these problems need to be solved simultaneously in order to reach human-level machine performance.\n\n\n== Approaches ==\nNo established unifying theory or paradigm guides AI research. Researchers disagree about many issues. A few of the most long-standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurobiology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering?\nCan intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of unrelated problems?\n\n\n=== Cybernetics and brain simulation ===\n\nIn the 1940s and 1950s, a number of researchers explored the connection between neurobiology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England. By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.\n\n\n=== Symbolic ===\n\nWhen access to digital computers became possible in the mid-1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford, and MIT, and as described below, each one developed its own style of research. John Haugeland named these symbolic approaches to AI \"good old fashioned AI\" or \"GOFAI\". During the 1960s, symbolic approaches had achieved great success at simulating high-level \"thinking\" in small demonstration programs. Approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.\nResearchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.\n\n\n==== Cognitive simulation ====\nEconomist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.\n\n\n==== Logic-based ====\nUnlike Simon and Newell, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem-solving, regardless of whether people used the same algorithms. His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning. Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.\n\n\n==== Anti-logic or scruffy ====\nResearchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad hoc solutions\u2014they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford). Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.\n\n\n==== Knowledge-based ====\nWhen computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications. This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software. A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules that illustrate AI. The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.\n\n\n=== Sub-symbolic ===\nBy the 1980s, progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems. Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.\n\n\n==== Embodied intelligence ====\nThis includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. Their work revived the non-symbolic point of view of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.\nWithin developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).\n\n\n==== Computational intelligence and soft computing ====\nInterest in neural networks and \"connectionism\" was revived by David Rumelhart and others in the middle of the 1980s. Artificial neural networks are an example of soft computing\u2014they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, Grey system theory, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.\n\n\n=== Statistical ===\nMuch of traditional GOFAI got bogged down on ad hoc patches to symbolic computation that worked on their own toy models but failed to generalize to real-world results. However, around the 1990s, AI researchers adopted sophisticated mathematical tools, such as hidden Markov models (HMM), information theory, and normative Bayesian decision theory to compare or to unify competing architectures. The shared mathematical language permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Compared with GOFAI, new \"statistical learning\" techniques such as HMM and neural networks were gaining higher levels of accuracy in many practical domains such as data mining, without necessarily acquiring a semantic understanding of the datasets. The increased successes with real-world data led to increasing emphasis on comparing different approaches against shared test data to see which approach performed best in a broader context than that provided by idiosyncratic toy models; AI research was becoming more scientific. Nowadays results of experiments are often rigorously measurable, and are sometimes (with difficulty) reproducible. Different statistical learning techniques have different limitations; for example, basic HMM cannot model the infinite possible combinations of natural language. Critics note that the shift from GOFAI to statistical learning is often also a shift away from explainable AI. In AGI research, some scholars caution against over-reliance on statistical learning, and argue that continuing research into GOFAI will still be necessary to attain general intelligence.\n\n\n=== Integrating the approaches ===\nIntelligent agent paradigm\nAn intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. The simplest intelligent agents are programs that solve specific problems. More complicated agents include human beings and organizations of human beings (such as firms). The paradigm allows researchers to directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given \"goal function\". An agent that solves a specific problem can use any approach that works\u2014some agents are symbolic and logical, some are sub-symbolic artificial neural networks and others may use new approaches. The paradigm also gives researchers a common language to communicate with other fields\u2014such as decision theory and economics\u2014that also use concepts of abstract agents. Building a complete agent requires researchers to address realistic problems of integration; for example, because sensory systems give uncertain information about the environment, planning systems must be able to function in the presence of uncertainty. The intelligent agent paradigm became widely accepted during the 1990s.Agent architectures and cognitive architectures\nResearchers have designed systems to build intelligent systems out of interacting intelligent agents in a multi-agent system. A hierarchical control system provides a bridge between sub-symbolic AI at its lowest, reactive levels and traditional symbolic AI at its highest levels, where relaxed time constraints permit planning and world modeling. Some cognitive architectures are custom-built to solve a narrow problem; others, such as Soar, are designed to mimic human cognition and to provide insight into general intelligence. Modern extensions of Soar are hybrid intelligent systems that include both symbolic and sub-symbolic components.\n\n\n== Tools ==\n\n\n== Applications ==\n\nAI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.High-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google Search), online assistants (such as Siri), image recognition in photographs, spam filtering, predicting flight delays, prediction of judicial decisions, targeting online advertisements,  and energy storageWith social media sites overtaking TV as a source for news for young people and news organizations increasingly reliant on social media platforms for generating distribution, major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.AI can also produce Deepfakes, a content-altering technology. ZDNet reports, \"It presents something that did not actually occur,\" Though 88% of Americans believe Deepfakes can cause more harm than good, only 47% of them believe they can be targeted. The boom of election year also opens public discourse to threats of videos of falsified politician media.\n\n\n== Philosophy and ethics ==\n\nThere are three philosophical questions related to AI \nWhether artificial general intelligence is possible; whether a machine can solve any problem that a human being can solve using intelligence, or if there are hard limits to what a machine can accomplish.\nWhether intelligent machines are dangerous; how humans can ensure that machines behave ethically and that they are used ethically.\nWhether a machine can have a mind, consciousness and mental states in the same sense that human beings do; if a machine can be sentient, and thus deserve certain rights \u2212 and if a machine can intentionally cause harm.\n\n\n=== The limits of artificial general intelligence ===\n\nAlan Turing's \"polite convention\"\nOne need not decide if a machine can \"think\"; one need only decide if a machine can act as intelligently as a human being. This approach to the philosophical problems associated with artificial intelligence forms the basis of the Turing test.The Dartmouth proposal\n\"Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.\" This conjecture was printed in the proposal for the Dartmouth Conference of 1956.Newell and Simon's physical symbol system hypothesis\n\"A physical symbol system has the necessary and sufficient means of general intelligent action.\" Newell and Simon argue that intelligence consists of formal operations on symbols. Hubert Dreyfus argues that, on the contrary, human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. (See Dreyfus' critique of AI.)G\u00f6delian arguments\nG\u00f6del himself, John Lucas (in 1961) and Roger Penrose (in a more detailed argument from 1989 onwards) made highly technical arguments that human mathematicians can consistently see the truth of their own \"G\u00f6del statements\" and therefore have computational abilities beyond that of mechanical Turing machines. However, some people do not agree with the \"G\u00f6delian arguments\".The artificial brain argument\nAn argument asserting that the brain can be simulated by machines and, because brains exhibit intelligence, these simulated brains must also exhibit intelligence \u2212 ergo, machines can be intelligent. Hans Moravec, Ray Kurzweil and others have argued that it is technologically feasible to copy the brain directly into hardware and software, and that such a simulation will be essentially identical to the original.The AI effect\nA hypothesis claiming that machines are already intelligent, but observers have failed to recognize it. For example, when Deep Blue beat Garry Kasparov in chess, the machine could be described as exhibiting intelligence. However, onlookers commonly discount the behavior of an artificial intelligence program by arguing that it is not \"real\" intelligence, with \"real\" intelligence being in effect defined as whatever behavior machines cannot do.\n\n\n=== Ethical machines ===\nMachines with intelligence have the potential to use their intelligence to prevent harm and minimize the risks; they may have the ability to use ethical reasoning to better choose their actions in the world. As such, there is a need for policy making to devise policies for and regulate artificial intelligence and robotics. Research in this area includes machine ethics, artificial moral agents, friendly AI and discussion towards building a human rights framework is also in talks.Joseph Weizenbaum in Computer Power and Human Reason wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.\n\n\n==== Artificial moral agents ====\nWendell Wallach introduced the concept of artificial moral agents (AMA) in his book Moral Machines For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\" and \"Can (Ro)bots Really Be Moral\". For Wallach, the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior, unlike the constraints which society may place on the development of AMAs.\n\n\n==== Machine ethics ====\n\nThe field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making. The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: \"Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems\u2014it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics.\" Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition \"Machine Ethics\" that stems from the AAAI Fall 2005 Symposium on Machine Ethics.\n\n\n==== Malevolent and friendly AI ====\n\nPolitical scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent. He argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of humanity and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\nOne proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI' and will be able to control subsequently developed AIs. Some question whether this kind of check could actually remain in place.\nLeading AI researcher Rodney Brooks writes, \"I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI and the enormity and complexity of building sentient volitional intelligence.\"Lethal autonomous weapons are of concern. Currently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers and drones.\n\n\n=== Machine consciousness, sentience and mind ===\n\nIf an AI system replicates all key aspects of human intelligence, will that system also be sentient\u2014will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.\n\n\n==== Consciousness ====\n\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however human subjective experience is difficult to explain.\nFor example, consider what happens when a person is shown a color swatch and identifies it, saying \"it's red\". The easy problem only requires understanding the machinery in the brain that makes it possible for a person to know that the color swatch is red. The hard problem is that people also know something else\u2014they also know what red looks like. (Consider that a person born blind can know that something is red without knowing what red looks like.) Everyone knows subjective experience exists, because they do it every day (e.g., all sighted people know what red looks like). The hard problem is explaining how the brain creates it, why it exists, and how it is different from knowledge and other aspects of the brain.\n\n\n==== Computationalism and functionalism ====\n\nComputationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\n\n\n==== Strong AI hypothesis ====\n\nThe philosophical position that John Searle has named \"strong AI\" states: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the \"mind\" might be.\n\n\n==== Robot rights ====\n\nIf a machine can be created that has intelligence, could it also feel? If it can feel, does it have the same rights as a human? This issue, now known as \"robot rights\", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature. Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights. The subject is profoundly discussed in the 2010 documentary film Plug & Pray, and many sci fi media such as Star Trek Next Generation, with the character of Commander Data, who fought being disassembled for research, and wanted to \"become human\", and the robotic holograms in Voyager.\n\n\n=== Superintelligence ===\n\nAre there limits to how intelligent machines\u2014or human-machine hybrids\u2014can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.\n\n\n==== Technological singularity ====\n\nIf research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement. The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario \"singularity\". Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Ray Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029 and predicts that the singularity will occur in 2045.\n\n\n==== Transhumanism ====\n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.\n\n\n== Impact ==\nThe long-term economic effects of AI are uncertain. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit, if productivity gains are redistributed. A 2017 study by PricewaterhouseCoopers sees the People\u2019s Republic of China gaining economically the most out of AI with 26,1% of GDP until 2030. A February 2020 European Union white paper on artificial intelligence advocated for artificial intelligence for economic benefits, including \"improving healthcare (e.g. making diagnosis more  precise,  enabling  better  prevention  of  diseases), increasing  the  efficiency  of  farming, contributing  to climate  change mitigation  and  adaptation, [and] improving  the  efficiency  of production systems through predictive maintenance\", while acknowledging potential risks.The relationship between automation and employment is complicated. While automation eliminates old jobs, it also creates new jobs through micro-economic and macro-economic effects. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. Author Martin Ford and others go further and argue that many jobs are routine, repetitive and (to an AI) predictable; Ford warns that these jobs may be automated in the next couple of decades, and that many of the new jobs may not be \"accessible to people with average capability\", even with retraining. Economists point out that in the past technology has tended to increase rather than reduce total employment, but acknowledge that \"we're in uncharted territory\" with AI.The potential negative effects of AI and automation were a major issue for Andrew Yang's 2020 presidential campaign in the United States. Irakli Beridze, Head of the Centre for Artificial Intelligence and Robotics at UNICRI, United Nations, has expressed that \"I think the dangerous applications for AI, from my point of view, would be criminals or large terrorist organizations using it to disrupt large processes or simply do pure harm. [Terrorists could cause harm] via digital warfare, or it could be a combination of robotics, drones, with AI and other things as well that could be really dangerous. And, of course, other risks come from things like job losses. If we have massive numbers of people losing jobs and don't find a solution, it will be extremely dangerous. Things like lethal autonomous weapons systems should be properly governed \u2014 otherwise there's massive potential of misuse.\"\n\n\n=== Risks of narrow AI ===\n\nWidespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to see how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.Some are concerned about algorithmic bias, that AI programs may unintentionally become biased after processing data that exhibits bias.  Algorithms already have numerous applications in legal systems. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants.\n\n\n=== Risks of general AI ===\n\nPhysicist Stephen Hawking, Microsoft founder Bill Gates, history professor Yuval Noah Harari, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\".\nThe development of full artificial intelligence could spell the end of the human race. Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\nIn his book Superintelligence, philosopher Nick Bostrom provides an argument that artificial intelligence will pose a threat to humankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's\u2014one example is an AI told to compute as many digits of pi as possible\u2014it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.  Bostrom also emphasizes the difficulty of fully conveying humanity's values to an advanced AI.  He uses the hypothetical example of giving an AI the goal to make humans smile to illustrate a misguided attempt.  If the AI in that scenario were to become superintelligent, Bostrom argues, it may resort to methods that most humans would find horrifying, such as inserting \"electrodes into the facial muscles of humans to cause constant, beaming grins\" because that would be an efficient way to achieve its goal of making humans smile.  In his book Human Compatible, AI researcher Stuart J. Russell echoes some of Bostrom's concerns while also proposing an approach to developing provably beneficial machines focused on uncertainty and deference to humans, possibly involving inverse reinforcement learning.Concern over risk from artificial intelligence has led to some high-profile donations and investments. A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1 billion to OpenAI, a nonprofit company aimed at championing responsible AI development. The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI. Other technology industry leaders believe that artificial intelligence is helpful in its current form and will continue to assist humans. Oracle CEO Mark Hurd has stated that AI \"will actually create more jobs, not less jobs\" as humans will be needed to manage AI systems. Facebook CEO Mark Zuckerberg believes AI will \"unlock a huge amount of positive things,\" such as curing disease and increasing the safety of autonomous cars. In January 2015, Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence. I think there is potentially a dangerous outcome there.\"For the danger of uncontrolled advanced AI to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching. Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.\n\n\n== Regulation ==\n\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms. The regulatory and  policy landscape for AI is an emerging issue in jurisdictions globally, including in the European Union. Regulation is considered necessary to both encourage AI and manage associated risks. Regulation of AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.\n\n\n== In fiction ==\n\nThought-capable artificial beings appeared as storytelling devices since antiquity,\nand have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune. In the 1980s, artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always an unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\n\n== See also ==\n\n\n== Explanatory notes ==\n\n\n== References ==\n\n\n=== AI textbooks ===\n\n\n=== History of AI ===\n\n\n=== Other sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\nThomason, Richmond. \"Logic and Artificial Intelligence\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nArtificial Intelligence, BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, Dec. 8, 2005)"
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Computer_architecture
Computer architecture - Wikipedia
In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation. In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.\n\n\n== History ==\nThe first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. When building the computer Z1 in 1936, Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept. Two other early and important examples are:\n\nJohn von Neumann's 1945 paper, First Draft of a Report on the EDVAC, which described an organization of logical elements; and\nAlan Turing's more detailed Proposed Electronic Calculator for the Automatic Computing Engine, also 1945 and which cited John von Neumann's paper.The term \u201carchitecture\u201d in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959. Johnson had the opportunity to write a proprietary research communication about the Stretch, an IBM-developed supercomputer for Los Alamos National Laboratory (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of \u201csystem architecture\u201d, a term that seemed more useful than \u201cmachine organization\u201d.Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, \u201cComputer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints.\u201dBrooks went on to help develop the IBM System/360 (now called the IBM zSeries) line of computers, in which \u201carchitecture\u201d became a noun defining \u201cwhat the user needs to know\u201d. Later, computer users came to use the term in many less explicit ways.The earliest computer architectures were designed on paper and then directly built into the final hardware form.\nLater, computer architecture prototypes were physically built in the form of a transistor\u2013transistor logic (TTL) computer\u2014such as the prototypes of the 6800 and the PA-RISC\u2014tested, and tweaked, before committing to the final hardware form.\nAs of the 1990s, new computer architectures are typically \"built\", tested, and tweaked\u2014inside some other computer architecture in a computer architecture simulator; or inside a FPGA as a soft microprocessor; or both\u2014before committing to the final hardware form.\n\n\n== Subcategories ==\nThe discipline of computer architecture has three main subcategories:\nInstruction set architecture (ISA): defines the machine code that a processor reads and acts upon as well as the word size, memory address modes, processor registers, and data type.\nMicroarchitecture: also known as \"computer organization\", this describes how a particular processor will implement the ISA. The size of a computer's CPU cache for instance, is an issue that generally has nothing to do with the ISA.\nSystems design: includes all of the other hardware components within a computing system, such as data processing other than the CPU (e.g., direct memory access), virtualization, and multiprocessingThere are other technologies in computer architecture. The following technologies are used in bigger companies like Intel, and were estimated in 2002 to count for 1% of all of computer architecture:\n\nMacroarchitecture: architectural layers more abstract than microarchitecture\nAssembly instruction set architecture: A smart assembler may convert an abstract assembly language common to a group of machines into slightly different machine language for different implementations.\nProgrammer-visible macroarchitecture: higher-level language tools such as compilers may define a consistent interface or contract to programmers using them, abstracting differences between underlying ISA, UISA, and microarchitectures. For example, the C, C++, or Java standards define different programmer-visible macroarchitectures.Microcode: microcode is software that translates instructions to run on a chip. It acts like a wrapper around the hardware, presenting a preferred version of the hardware's instruction set interface. This instruction translation facility gives chip designers flexible options: E.g. 1. A new improved version of the chip can use microcode to present the exact same instruction set as the old chip version, so all software targeting that instruction set will run on the new chip without needing changes. E.g. 2. Microcode can present a variety of instruction sets for the same underlying chip, allowing it to run a wider variety of software.UISA: User Instruction Set Architecture, refers to one of three subsets of the RISC CPU instructions provided by PowerPC RISC Processors. The UISA subset, are those RISC instructions of interest to application developers. The other two subsets are VEA (Virtual Environment Architecture) instructions used by virtualisation system developers, and OEA (Operating Environment Architecture) used by Operation System developers.Pin architecture: The hardware functions that a microprocessor should provide to a hardware platform, e.g., the x86 pins A20M, FERR/IGNNE or FLUSH. Also, messages that the processor should emit so that external caches can be invalidated (emptied). Pin architecture functions are more flexible than ISA functions because external hardware can adapt to new encodings, or change from a pin to a message. The term \"architecture\" fits, because the functions must be provided for compatible systems, even if the detailed method changes.\n\n\n== Roles ==\n\n\n=== Definition ===\nComputer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. The case of instruction set architecture can be used to illustrate the balance of these competing factors. More complex instruction sets enable programmers to write more space efficient programs, since a single instruction can encode some higher-level abstraction (such as the x86 Loop instruction). However, longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways.\nThe implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with compilers, operating systems to logic design, and packaging.\n\n\n=== Instruction set architecture ===\n\nAn instruction set architecture (ISA) is the interface between the computer's software and hardware and also can be viewed as the programmer's view of the machine. Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand.\nBesides instructions, the ISA defines items in the computer that are available to a program\u2014e.g., data types, registers, addressing modes, and memory.  Instructions locate these available items with register indexes (or names) and memory addressing modes.\nThe ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler.  An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form.  Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs.\nISAs vary in quality and completeness.  A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time).  Memory organization defines how instructions interact with the memory, and how memory interacts with itself.\nDuring design emulation, emulators can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine whether a particular ISA is meeting its goals.\n\n\n=== Computer organization ===\n\nComputer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite a detailed analysis of the computer's organization.  For example, in a SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.\nComputer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well.  For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.\n\n\n=== Implementation ===\nOnce an instruction set and micro-architecture have been designed, a practical machine must be developed. This design process is called the implementation. Implementation is usually not considered architectural design, but rather hardware design engineering. Implementation can be further broken down into several steps:\n\nLogic implementation designs the circuits required at a logic-gate level.\nCircuit implementation does transistor-level designs of basic elements (e.g., gates, multiplexers, latches) as well as of some larger blocks (ALUs, caches etc.) that may be implemented at the logic-gate level, or even at the physical level if the design calls for it.\nPhysical implementation draws physical circuits.  The different circuit components are placed in a chip floorplan or on a board and the wires connecting them are created.\nDesign validation tests the computer as a whole to see if it works in all situations and all timings. Once the design validation process starts, the design at the logic level are tested using logic emulators. However, this is usually too slow to run a realistic test.  So, after making corrections based on the first test, prototypes are constructed using Field-Programmable Gate-Arrays (FPGAs). Most hobby projects stop at this stage.  The final step is to test prototype integrated circuits, which may require several redesigns.For CPUs, the entire implementation process is organized differently and is often referred to as CPU design.\n\n\n== Design goals ==\nThe exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.\nThe most common scheme does an in-depth power analysis and figures out how to keep power consumption low while maintaining adequate performance.\n\n\n=== Performance ===\nModern computer performance is often described in instructions per cycle (IPC), which measures the efficiency of the architecture at any clock frequency; a faster IPC rate means the computer is faster. Older computers had IPC counts as low as 0.1 while modern processors easily reach near 1. Superscalar processors may reach three to five IPC by executing several instructions per clock cycle.Counting machine-language instructions would be misleading because they can do varying amounts of work in different ISAs. The \"instruction\" in the standard measurements is not a count of the ISA's machine-language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture.\nMany people used to measure a computer's speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance.\nOther factors influence speed, such as the mix of functional units, bus speeds, available memory, and the type and order of instructions in the programs.\nThere are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time.  Interrupt latency is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data).\nPerformance is affected by a very wide range of design choices \u2014 for example, pipelining a processor usually makes latency worse, but makes throughput better. Computers that control machinery usually need low interrupt latencies. These computers operate in a real-time environment and fail if an operation is not completed in a specified amount of time. For example, computer-controlled anti-lock brakes must begin braking within a predictable and limited time period after the brake pedal is sensed or else failure of the brake will occur.\nBenchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn't be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don't offer similar advantages to general tasks.\n\n\n=== Power efficiency ===\n\nPower efficiency is another important measurement in modern computers. A higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt).\nModern circuits have less power required per transistor as the number of transistors per chip grows. This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible. In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency.\n\n\n=== Shifts in market demand ===\nIncreases in clock frequency have grown more slowly over the past few years, compared to power reduction improvements. This has been driven by the end of Moore's Law and demand for longer battery life and reductions in size for mobile technology. This change in focus from higher clock rates to power consumption and miniaturization can be shown by the significant reductions in power consumption, as much as 50%, that were reported by Intel in their release of the Haswell microarchitecture; where they dropped their power consumption benchmark from 30 to 40 watts down to 10-20 watts. Comparing this to the processing speed increase of 3 GHz to 4 GHz (2002 to 2006) it can be seen that the focus in research and development are shifting away from clock frequency and moving towards consuming less power and taking up less space.\n\n\n== See also ==\n\nComparison of CPU architectures\nComputer hardware\nCPU design\nFloating point\nHarvard architecture (Modified)\nDataflow architecture\nTransport triggered architecture\nReconfigurable computing\nInfluence of the IBM PC on the personal computer market\nOrthogonal instruction set\nSoftware architecture\nvon Neumann architecture\nFlynn's taxonomy\n\n\n== References ==\n\n\n== Sources ==\nJohn L. Hennessy and David Patterson (2006). Computer Architecture: A Quantitative Approach (Fourth ed.). Morgan Kaufmann. ISBN 978-0-12-370490-0.\nBarton, Robert S., \"Functional Design of Computers\", Communications of the ACM 4(9): 405 (1961).\nBarton, Robert S., \"A New Approach to the Functional Design of a Digital Computer\", Proceedings of the Western Joint Computer Conference, May 1961, pp. 393\u2013396. About the design of the Burroughs B5000 computer.\nBell, C. Gordon; and Newell, Allen (1971). \"Computer Structures: Readings and Examples\", McGraw-Hill.\nBlaauw, G.A., and Brooks, F.P., Jr., \"The Structure of System/360, Part I-Outline of the Logical Structure\", IBM Systems Journal, vol. 3, no. 2, pp. 119\u2013135, 1964.\nTanenbaum, Andrew S. (1979). Structured Computer Organization. Englewood Cliffs, New Jersey: Prentice-Hall. ISBN 0-13-148521-0.\n\n\n== External links ==\nISCA: Proceedings of the International Symposium on Computer Architecture\nMicro: IEEE/ACM International Symposium on Microarchitecture\nHPCA: International Symposium on High Performance Computer Architecture\nASPLOS: International Conference on Architectural Support for Programming Languages and Operating Systems\nACM Transactions on Architecture and Code Optimization\nIEEE Transactions on Computers\nThe von Neumann Architecture of Computer Systems"
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/History_of_computer_science
History of computer science - Wikipedia
The history of computer science began long before our modern discipline of computer science, usually appearing in forms like mathematics or physics. Developments in previous centuries alluded to the discipline that we now know as computer science. This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.\n\n\n== Prehistory ==\nThe earliest known tool for use in computation was the abacus, developed in the period between 2700 and 2300 BCE in Sumer. The Sumerians' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system. Its original style of usage was by lines drawn in sand with pebbles. Abaci of a more modern design are still used as calculation tools today, such as the Chinese abacus.In the 5th century BC in ancient India, the grammarian P\u0101\u1e47ini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.The Antikythera mechanism is believed to be an early mechanical analog computer.  It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.Mechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Ab\u016b Rayh\u0101n al-B\u012br\u016bn\u012b, and the torquetum by Jabir ibn Aflah. According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus. Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Ban\u016b M\u016bs\u0101 brothers, and Al-Jazari's programmable humanoid automata and castle clock, which is considered to be the first programmable analog computer. Technological artifacts of similar complexity appeared in 14th century Europe, with mechanical astronomical clocks.When John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624. Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria. Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.\n\n\n== Binary logic ==\nIn 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. In his system, the ones and zeros also represent true and false values or on and off states. But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled.By this time, the first mechanical devices driven by a binary pattern had been invented. The industrial revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems.\n\n\n== Creation of the computer ==\nBefore the 1920s, computers (sometimes computors) were human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Many of these clerks who served as human computers were women. Some performed astronomical calculations for calendars, others ballistic tables for the military.After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.Machines that computed with continuous values became known as the analog kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.The phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described \"purely mechanical.\" The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.\n\n\n== Emergence of a discipline ==\n\n\n=== Charles Babbage and Ada Lovelace ===\n\nCharles Babbage is often regarded as one of the first pioneers of computing. Beginning in the 1810s, Babbage had a vision of mechanically computing numbers and tables. Putting this into reality, Babbage designed a calculator to compute numbers up to 8 decimal points long. Continuing with the success of this idea, Babbage worked to develop a machine that could compute numbers with up to 20 decimal places. By the 1830s, Babbage had devised a plan to develop a machine that could use punched cards to perform arithmetical operations. The machine would store numbers in memory units, and there would be a form of sequential control.  This means that one operation would be carried out before another in such a way that the machine would produce an answer and not fail. This machine was to be known as the \u201cAnalytical Engine\u201d, which was the first true representation of what is the modern computer.Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his \u201cAnalytical Engine\u201d, the first mechanical computer. During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers. Moreover, Lovelace's work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not. While she was never able to see the results of her work, as the \u201cAnalytical Engine\u201d was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.\n\n\n=== Alan Turing and the Turing machine ===\n\nThe mathematical foundations of modern computer science began to be laid by Kurt G\u00f6del with his incompleteness theorem (1931). In this theorem, he showed that there were limits to what could be proved and disproved within a formal system. This led to work by G\u00f6del and others to define and describe these formal systems, including concepts such as mu-recursive functions and lambda-definable functions.In 1936  Alan Turing and Alonzo Church independently, and also together, introduced the formalization of an algorithm, with limits on what can be computed, and a \"purely mechanical\" model for computing. This became the Church\u2013Turing thesis, a hypothesis about the nature of mechanical calculation devices, such as electronic computers. The thesis claims that any calculation that is possible can be performed by an algorithm running on a computer, provided that sufficient time and storage space are available.In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine. This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use. These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability. If a Turing machine can complete the task, it is considered Turing computable.The Los Alamos physicist Stanley Frankel, has described John von Neumann's view of the fundamental importance of Turing's 1936 paper, in a letter:\n I know that in or about 1943 or \u201844 von Neumann was well aware of the fundamental importance of Turing's paper of 1936\u2026 Von Neumann introduced me to that paper and at his urging I studied it with care. Many people have acclaimed von Neumann as the \"father of the computer\" (in a modern sense of the term) but I am sure that he would never have made that mistake himself. He might well be called the midwife, perhaps, but he firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing...\n\n\n=== Akira Nakashima and switching circuit theory ===\nIn an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits. During 1880\u201381 he showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but this work on it was unpublished until 1933. The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called Peirce's arrow. Consequently, these gates are sometimes called universal logic gates.Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as a logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935\u201338).\nUp to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor.  This changed with NEC engineer Akira Nakashima's switching circuit theory in the 1930s. From 1934 to 1936, Nakashima published a series of papers showing that the two-valued Boolean algebra, which he discovered independently (he was unaware of George Boole's work until 1938), can describe the operation of switching circuits. This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.Nakashima's work was later cited and elaborated on in Claude Elwood Shannon's seminal 1937 master's thesis \"A Symbolic Analysis of Relay and Switching Circuits\". While taking an undergraduate philosophy class, Shannon had been exposed to Boole's work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems. His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.\n\n\n=== Early computer hardware ===\nThe world's first electronic digital computer, the Atanasoff\u2013Berry computer, was built on the Iowa State campus from 1939 through 1942 by John V. Atanasoff, a professor of physics and mathematics, and Clifford Berry, an engineering graduate student.\nIn 1941, Konrad Zuse developed the world's first functional program-controlled computer, the Z3. In 1998, it was shown to be Turing-complete in principle. Zuse also developed the S2 computing machine, considered the first process control computer. He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world's first commercial computer.  In 1946, he designed the first high-level programming language, Plankalk\u00fcl.In 1948, the Manchester Baby was completed; it was the world's first electronic digital computer that ran programs stored in its memory, like almost all modern computers. The influence on Max Newman of Turing's seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Baby.In 1950, Britain's National Physical Laboratory completed Pilot ACE, a small scale programmable computer, based on Turing's philosophy. With an operating speed of 1 MHz, the Pilot Model ACE was for some time the fastest computer in the world. Turing's design for ACE had much in common with today's RISC architectures and it called for a high-speed memory of roughly the same capacity as an early Macintosh computer, which was enormous by the standards of his day. Had Turing's ACE been built as planned and in full, it would have been in a different league from the other early computers.\n\n\n=== Shannon and information theory ===\nClaude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit.  This work is one of the theoretical foundations for many areas of study, including data compression and cryptography.\n\n\n=== Wiener and cybernetics ===\nFrom experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for \"steersman.\" He published \"Cybernetics\" in 1948, which influenced artificial intelligence. Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.The first actual computer bug was a moth. It was stuck in between the relays on the Harvard Mark II.\nWhile the invention of the term 'bug' is often but erroneously attributed to Grace Hopper, a future rear admiral in the U.S. Navy, who supposedly logged the \"bug\" on September 9, 1945, most other accounts conflict at least with these details. According to these accounts, the actual date was September 9, 1947 when operators filed this 'incident' \u2014 along with the insect and the notation \"First actual case of bug being found\" (see software bug for details).\n\n\n=== John von Neumann and the von Neumann architecture ===\n\nIn 1946, a model for computer architecture was introduced and became known as Von Neumann architecture. Since 1950, the von Neumann model provided uniformity in subsequent computer designs. The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space.  The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU). In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.Von Neumann's machine design uses a RISC (Reduced instruction set computing) architecture, which means the instruction set uses a total of 21 instructions to perform all tasks. (This is in contrast to CISC, complex instruction set computing, instruction sets which have more instructions from which to choose.)  With von Neumann architecture, main memory along with the accumulator (the register that holds the result of logical operations) are the two memories that are addressed. Operations can be carried out as simple arithmetic (these are performed by the ALU and include addition, subtraction, multiplication and division), conditional branches (these are more commonly seen now as if statements or while loops. The branches serve as go to statements), and logical moves between the different components of the machine, i.e., a move from the accumulator to memory or vice versa. Von Neumann architecture accepts fractions and instructions as data types. Finally, as the von Neumann architecture is a simple one, its register management is also simple. The architecture uses a set of seven registers to manipulate and interpret fetched data and instructions. These registers include the \"IR\" (instruction register), \"IBR\" (instruction buffer register), \"MQ\" (multiplier quotient register), \"MAR\" (memory address register), and \"MDR\" (memory data register).\"  The architecture also uses a program counter (\"PC\") to keep track of where in the program the machine is.\n\n\n=== John McCarthy, Marvin Minsky and Artificial intelligence ===\n\n\n=== Transistors and the computer revolution ===\n\nThe concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947, which was followed by Shockley's bipolar junction transistor in 1948. In 1953, the University of Manchester built the first transistorized computer, called the Transistor Computer. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications.The metal\u2013oxide\u2013silicon field-effect transistor (MOSFET), also known as the MOS transistor, was invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses. With its high scalability, and much lower power consumption and higher density than bipolar junction transistors, the MOSFET made it possible to build high-density integrated circuits. The MOSFET later led to the microcomputer revolution, and became the driving force behind the computer revolution. The MOSFET is the most widely used transistor in computers, and is the fundamental building block of digital electronics.\n\n\n== See also ==\nComputer Museum\nList of computer term etymologies, the origins of computer science words\nList of pioneers in computer science\nHistory of computing\nHistory of computing hardware\nHistory of software\nHistory of personal computers\nTimeline of algorithms\nTimeline of women in computing\nTimeline of computing 2020\u20132029\n\n\n== References ==\n\n\n=== Sources ===\nEvans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.\nGrier, David Alan (2013). When Computers Were Human. Princeton: Princeton University Press. ISBN 9781400849369 \u2013 via Project MUSE.\n\n\n== Further reading ==\nTedre, Matti (2014). The Science of Computing: Shaping a Discipline. Taylor and Francis / CRC Press. ISBN 978-1-4822-1769-8.\nKak, Subhash : Computing Science in Ancient India; Munshiram Manoharlal Publishers Pvt. Ltd (2001)\nThe Development of Computer Science: A Sociocultural Perspective Matti Tedre's Ph.D. Thesis, University of Joensuu (2006)\nCeruzzi, Paul E. (1998). A History of a Modern Computing. The MIT Press. ISBN 978-0-262-03255-1.\nCopeland, B. Jack. \"The Modern History of Computing\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\n\n\n== External links ==\nComputer History Museum\nComputers: From the Past to the Present\nThe First \"Computer Bug\" at the Naval History and Heritage Command Photo Archives.\nBitsavers, an effort to capture, salvage, and archive historical computer software and manuals from minicomputers and mainframes of the 1950s, 1960s, 1970s, and 1980s\nOral history interviews"
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Outline_of_computer_science
Outline of computer science - Wikipedia
Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.\n\n\n== What is computer science? ==\nComputer science can be described as all of the following:\n\nAcademic discipline\nScience\nApplied science\n\n\n== Subfields ==\n\n\n=== Mathematical foundations ===\nCoding theory \u2013 Useful in networking and other areas where computers communicate with each other.\nGame theory \u2013 Useful in artificial intelligence and cybernetics.\nDiscrete Mathematics\nGraph theory \u2013 Foundations for data structures and searching algorithms.\nMathematical logic \u2013 Boolean logic and other ways of modeling logical queries; the uses and limitations of formal proof methods\nNumber theory \u2013 Theory of the integers.  Used in cryptography as well as a test domain in artificial intelligence.\n\n\n=== Algorithms and data structures ===\nAlgorithms \u2013 Sequential and parallel computational procedures for solving a wide range of problems.\nData structures \u2013 The organization and manipulation of data.\n\n\n=== Artificial intelligence ===\nOutline of artificial intelligence\n\nArtificial intelligence \u2013 The implementation and study of systems that exhibit an autonomous intelligence or behavior of their own.\nAutomated reasoning \u2013 Solving engines, such as used in Prolog, which produce steps to a result given a query on a fact and rule database, and automated theorem provers that aim to prove mathematical theorems with some assistance from a programmer.\nComputer vision \u2013 Algorithms for identifying three-dimensional objects from a two-dimensional picture.\nSoft computing, the use of inexact solutions for otherwise extremely difficult problems:\nMachine learning - Automated creation of a set of rules and axioms based on input.\nEvolutionary computing - Biologically inspired algorithms.\nNatural language processing - Building systems and algorithms that analyze, understand, and generate natural (human) languages.\nRobotics \u2013 Algorithms for controlling the behaviour of robots.\n\n\n=== Communication and security ===\nNetworking \u2013 Algorithms and protocols for reliably communicating data across different shared or dedicated media, often including error correction.\nComputer security \u2013 Practical aspects of securing computer systems and computer networks.\nCryptography \u2013 Applies results from complexity, probability, algebra and number theory to invent and break codes, and analyze the security of cryptographic protocols.\n\n\n=== Computer architecture ===\nComputer architecture \u2013 The design, organization, optimization and verification of a computer system, mostly about CPUs and Memory subsystem (and the bus connecting them).\nOperating systems \u2013 Systems for managing computer programs and providing the basis of a usable system.\n\n\n=== Computer graphics ===\nComputer graphics \u2013 Algorithms both for generating visual images synthetically, and for integrating or altering visual and spatial information sampled from the real world.\nImage processing \u2013 Determining information from an image through computation.\nInformation visualization \u2013 Methods for representing and displaying abstract data to facilitate human interaction for exploration and understanding.\n\n\n=== Concurrent, parallel, and distributed systems ===\nParallel computing - The theory and practice of simultaneous computation; data safety in any multitasking or multithreaded environment.\nConcurrency (computer science) \u2013 Computing using multiple concurrent threads of execution, devising algorithms for solving problems on multiple processors to achieve maximal speed-up compared to sequential execution.\nDistributed computing \u2013 Computing using multiple computing devices over a network to accomplish a common objective or task and thereby reducing the latency involved in single processor contributions for any task.\n\n\n=== Databases ===\nOutline of databases\n\nRelational databases \u2013 the set theoretic and algorithmic foundation of databases.\nStructured Storage - non-relational databases such as NoSQL databases.\nData mining \u2013 Study of algorithms for searching and processing information in documents and databases; closely related to information retrieval.\n\n\n=== Programming languages and compilers ===\nCompiler theory \u2013 Theory of compiler design, based on Automata theory.\nProgramming language pragmatics \u2013 Taxonomy of programming languages, their strength and weaknesses. Various programming paradigms, such as object-oriented programming.\nProgramming language theory\nFormal semantics \u2013 rigorous mathematical study of the meaning of programs.\nType theory \u2013 Formal analysis of the types of data, and the use of these types to understand properties of programs \u2014 especially program safety.\n\n\n=== Scientific computing ===\nComputational science \u2013 constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems.\nNumerical analysis \u2013 Approximate numerical solution of mathematical problems such as root-finding, integration, the solution of ordinary differential equations; the approximation of special functions.\nSymbolic computation \u2013 Manipulation and solution of expressions in symbolic form, also known as Computer algebra.\nComputational physics \u2013 Numerical simulations of large non-analytic systems\nComputational chemistry \u2013 Computational modelling of theoretical chemistry in order to determine chemical structures and properties\nBioinformatics and Computational biology \u2013 The use of computer science to maintain, analyse, store biological data and to assist in solving biological problems such as Protein folding, function prediction and Phylogeny.\nComputational neuroscience \u2013 Computational modelling of neurophysiology.\n\n\n=== Software engineering ===\nOutline of software engineering\n\nFormal methods \u2013 Mathematical approaches for describing and reasoning about software design.\nSoftware engineering \u2013 The principles and practice of designing, developing, and testing programs, as well as proper engineering practices.\nAlgorithm design \u2013 Using ideas from algorithm theory to creatively design solutions to real tasks.\nComputer programming \u2013 The practice of using a programming language to implement algorithms.\nHuman\u2013computer interaction \u2013 The study and design of computer interfaces that people use.\nReverse engineering \u2013 The application of the scientific method to the understanding of arbitrary existing software.\n\n\n=== Theory of computation ===\n\nAutomata theory \u2013 Different logical structures for solving problems.\nComputability theory \u2013 What is calculable with the current models of computers. Proofs developed by Alan Turing and others provide insight into the possibilities of what may be computed and what may not.\nList of unsolved problems in computer science\nComputational complexity theory \u2013 Fundamental bounds (especially time and storage space) on classes of computations.\nQuantum computing theory \u2013 Explores computational models involving quantum superposition of bits.\n\n\n== History ==\nHistory of computer science\nList of pioneers in computer science\n\n\n== Professions ==\nProgrammer\nTeacher/Professor\nSoftware engineer\nSoftware architect\nSoftware developer\nSoftware tester\nHardware engineer\nData analyst\nInteraction designer\nNetwork administrator\n\n\n== Data and data structures ==\nData structure\nData type\nAssociative array and Hash table\nArray\nList\nTree\nString\nMatrix (computer science)\nDatabase\n\n\n== Programming paradigms ==\nImperative programming/Procedural programming\nFunctional programming\nLogic programming\nObject oriented programming\nClass\nInheritance\nObject\n\n\n== See also ==\nAbstraction\nBig O notation\nClosure\nCompiler\nCognitive science\n\n\n== External links ==\n\nOutline of computer science at Curlie\nACM report on a recommended computer science curriculum (2008)\nDirectory of free university lectures in Computer Science\nCollection of Computer Science Bibliographies\nPhotographs of computer scientists (Bertrand Meyer's gallery)"
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Glossary_of_computer_science
Glossary of computer science - Wikipedia
This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.\n\n\n== A ==\nabstract data type (ADT)\nA mathematical model for data types in which a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data from the point of view of an implementer rather than a user.\n\nabstract method\nOne with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method. Abstract methods are used to specify interfaces in some computer languages.\n\nabstraction\n1.  In software engineering and computer science, the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest; it is also very similar in nature to the process of generalization.\n2.  The result of this process: an abstract concept-object created by keeping common features or attributes to various concrete objects or systems of study.\n\nagent architecture\nA blueprint for software agents and intelligent control systems depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures.\n\nagent-based model (ABM)\nA class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness.\n\naggregate function\nIn database management, a function in which the values of multiple rows are grouped together to form a single value of more significant meaning or measurement, such as a set, a bag, or a list.\n\nagile software development\nAn approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, early delivery, and continual improvement, and it encourages rapid and flexible response to change.\n\nalgorithm\nAn unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks. They are ubiquitous in computing technologies.\n\nalgorithm design\nA method or mathematical process for problem-solving and for engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.\n\nalgorithmic efficiency\nA property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.\n\nAmerican Standard Code for Information Interchange (ASCII)\nA character encoding standard for electronic communications. ASCII codes represent text in computers, telecommunications equipment, and other devices. Most modern character-encoding schemes are based on ASCII, although they support many additional characters.\n\napplication programming interface (API)\nA set of subroutine definitions, communication protocols, and tools for building software. In general terms, it is a set of clearly defined methods of communication among various components. A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer.\n\napplication software\nAlso simply application or app.\nComputer software designed to perform a group of coordinated functions, tasks, or activities for the benefit of the user. Common examples of applications include word processors, spreadsheets, accounting applications, web browsers, media players, aeronautical flight simulators, console games, and photo editors. This contrasts with system software, which is mainly involved with managing the computer's most basic running operations, often without direct input from the user. The collective noun application software refers to all applications collectively.\n\narray data structure\nAlso simply array.\nA data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called a one-dimensional array.\n\nartifact\nOne of many kinds of tangible by-products produced during the development of software. Some artifacts (e.g. use cases, class diagrams, and other Unified Modeling Language (UML) models, requirements, and design documents) help describe the function, architecture, and design of software. Other artifacts are concerned with the process of development itself\u2014such as project plans, business cases, and risk assessments.\n\nartificial intelligence (AI)\nAlso machine intelligence.\nIntelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science, AI research is defined as the study of \"intelligent agents\": devices capable of perceiving their environment and taking actions that maximize the chance of successfully achieving their goals. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".\n\nASCII\nSee American Standard Code for Information Interchange.\n\nassertion\nIn computer programming, a statement that a predicate (Boolean-valued function, i.e. a true\u2013false expression) is always true at that point in code execution. It can help a programmer read the code, help a compiler compile it, or help the program detect its own defects. For the latter, some programs check assertions by actually evaluating the predicate as they run and if it is not in fact true \u2013 an assertion failure \u2013 the program considers itself to be broken and typically deliberately crashes or throws an assertion failure exception.\n\nassociative array\nAn associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection.\n\nOperations associated with this data type allow:the addition of a pair to the collection\nthe removal of a pair from the collection\nthe modification of an existing pair\nthe lookup of a value associated with a particular key\n\nautomata theory\nThe study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science).\n\nautomated reasoning\nAn area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.\n\n\n== B ==\nbandwidth\nThe maximum rate of data transfer across a given path. Bandwidth may be characterized as network bandwidth, data bandwidth, or digital bandwidth.\n\nBayesian programming\nA formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.\n\nbenchmark\nThe act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it. The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves.\n\nbest, worst and average case\nExpressions of what the resource usage is at least, at most, and on average, respectively, for a given algorithm. Usually the resource being considered is running time, i.e. time complexity, but it could also be memory or some other resource. Best case is the function which performs the minimum number of steps on input data of n elements; worst case is the function which performs the maximum number of steps on input data of size n; average case is the function which performs an average number of steps on input data of n elements.\n\nbig data\nA term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.\n\nbig O notation\nA mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann\u2013Landau notation or asymptotic notation.\n\nbinary number\nIn mathematics and digital electronics, a number expressed in the base-2 numeral system or binary numeral system, which uses only two symbols: typically 0 (zero) and 1 (one).\n\nbinary search algorithm\nAlso simply binary search, half-interval search, logarithmic search, or binary chop.\nA search algorithm that finds the position of a target value within a sorted array.\n\nbinary tree\nA tree data structure in which each node has at most two children, which are referred to as the left child and the right child. A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set. Some authors allow the binary tree to be the empty set as well.\n\nbioinformatics\nAn interdisciplinary field that combines biology, computer science, information engineering, mathematics, and statistics to develop methods and software tools for analyzing and interpreting biological data. Bioinformatics is widely used for in silico analyses of biological queries using mathematical and statistical techniques.\n\nbit\nA basic unit of information used in computing and digital communications; a portmanteau of binary digit. A binary digit can have one of two possible values, and may be physically represented with a two-state device. These state values are most commonly represented as either a 0or1.\n\nbit rate (R)\n\nAlso bitrate.\nIn telecommunications and computing, the number of bits that are conveyed or processed per unit of time.\n\nblacklist\nAlso block list.\nIn computing, a basic access control mechanism that allows through all elements (email addresses, users, passwords, URLs, IP addresses, domain names, file hashes, etc.), except those explicitly mentioned in a list of prohibited elements. Those items on the list are denied access. The opposite is a whitelist, which means only items on the list are allowed through whatever gate is being used while all other elements are blocked. A greylist contains items that are temporarily blocked (or temporarily allowed) until an additional step is performed.\n\nBMP file format\nAlso bitmap image file, device independent bitmap (DIB) file format, or simply bitmap.\nA raster graphics image file format used to store bitmap digital images independently of the display device (such as a graphics adapter), used especially on Microsoft Windows and OS/2 operating systems.\n\nBoolean data type\nA data type that has one of two possible values (usually denoted true and false), intended to represent the two truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid-19th century. The Boolean data type is primarily associated with conditional statements, which allow different actions by changing control flow depending on whether a programmer-specified Boolean condition evaluates to true or false. It is a special case of a more general logical data type (see probabilistic logic)\u2014i.e. logic need not always be Boolean.\n\nBoolean expression\nAn expression used in a programming language that returns a Boolean value when evaluated, that is one of true or false. A Boolean expression may be composed of a combination of the Boolean constants true or false, Boolean-typed variables, Boolean-valued operators, and Boolean-valued functions.\n\nBoolean algebra\nIn mathematics and mathematical logic, the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively. Contrary to elementary algebra, where the values of the variables are numbers and the prime operations are addition and multiplication, the main operations of Boolean algebra are the conjunction and (denoted as \u2227), the disjunction or (denoted as \u2228), and the negation not (denoted as \u00ac). It is thus a formalism for describing logical relations in the same way that elementary algebra describes numeric relations.\n\nbyte\nA unit of digital information that most commonly consists of eight bits, representing a binary number. Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures.\n\nbooting\nThe procedures implemented in starting up a computer or computer appliance until it can be used. It can be initiated by hardware such as a button press or by a software command. After the power is switched on, the computer is relatively dumb and can read only part of its storage called read-only memory. There, a small program is stored called firmware. It does power-on self-tests and, most importantly, allows access to other types of memory like a hard disk and main memory. The firmware loads bigger programs into the computer's main memory and runs it.\n\n\n== C ==\ncallback\nAlso a call-after function.\nAny executable code that is passed as an argument to other code that is expected to \"call back\" (execute) the argument at a given time. This execution may be immediate, as in a synchronous callback, or it might happen at a later time, as in an asynchronous callback.\n\ncentral processing unit (CPU)\nThe electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions. The computer industry has used the term \"central processing unit\" at least since the early 1960s. Traditionally, the term \"CPU\" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.\n\ncharacter\nA unit of information that roughly corresponds to a grapheme, grapheme-like unit, or symbol, such as in an alphabet or syllabary in the written form of a natural language.\n\ncipher\nAlso cypher.\nIn cryptography, an algorithm for performing encryption or decryption\u2014a series of well-defined steps that can be followed as a procedure.\n\nclass\nIn object-oriented programming, an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated.\n\nclass-based programming\nAlso class-orientation.\nA style of object-oriented programming (OOP) in which inheritance occurs via defining \"classes\" of objects, instead of via the objects alone. Compare prototype-based programming.\n\nclass-orientation\nA style of Object-oriented programming (OOP) in which inheritance occurs via defining classes of objects, instead of inheritance occurring via the objects alone (compare prototype-based programming).\n\nclient\nA piece of computer hardware or software that accesses a service made available by a server. The server is often (but not always) on another computer system, in which case the client accesses the service by way of a network. The term applies to the role that programs or devices play in the client\u2013server model.\n\ncleanroom software engineering\nA software development process intended to produce software with a certifiable level of reliability. The cleanroom process was originally developed by Harlan Mills and several of his colleagues including Alan Hevner at IBM. The focus of the cleanroom process is on defect prevention, rather than defect removal.\n\nclosure\nAlso lexical closure or function closure.\nA technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function together with an environment.\n\ncloud computing\nShared pools of configurable computer system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet.  Cloud computing relies on sharing of resources to achieve coherence and economies of scale, similar to a public utility.\n\ncode library\nA collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.\n\ncoding\nComputer programming is the process of designing and building an executable computer program for accomplishing a specific computing task. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.\n\ncoding theory\nThe study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines\u2014such as information theory, electrical engineering, mathematics, linguistics, and computer science\u2014for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.\n\ncognitive science\nThe interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology.\n\ncollection\nA collection or container is a grouping of some variable number of data items (possibly zero) that have some shared significance to the problem being solved and need to be operated upon together in some controlled fashion.  Generally, the data items will be of the same type or, in languages supporting inheritance, derived from some common ancestor type. A collection is a concept applicable to abstract data types, and does not prescribe a specific implementation as a concrete data structure, though often there is a conventional choice (see Container for type theory discussion).\n\ncomma-separated values (CSV)\nA delimited text file that uses a comma to separate values. A CSV file stores tabular data (numbers and text) in plain text.  Each line of the file is a data record.  Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format.\n\ncompiler\nA computer program that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower-level language (e.g. assembly language, object code, or machine code) to create an executable program.\n\ncomputability theory\nalso known as recursion theory, is a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.\n\ncomputation\nAny type of calculation that includes both arithmetical and non-arithmetical steps and follows a well-defined model, e.g. an algorithm. The study of computation is paramount to the discipline of computer science.\n\ncomputational biology\nInvolves the development and application of data-analytical and theoretical methods, mathematical modelling and computational simulation techniques to the study of biological, ecological, behavioural, and social systems. The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science, and evolution.  Computational biology is different from biological computing, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers.\n\ncomputational chemistry\nA branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids.\n\ncomputational complexity theory\nA subfield of computational science which focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\n\ncomputational model\nA mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation.\n\ncomputational neuroscience\nAlso theoretical neuroscience or mathematical neuroscience.\nA branch of neuroscience which employs mathematical models, theoretical analysis, and abstractions of the brain to understand the principles that govern the development, structure, physiology, and cognitive abilities of the nervous system.\n\ncomputational physics\nIs the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science.\n\ncomputational science\nAlso scientific computing and scientific computation (SC).\nAn interdisciplinary field that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core it involves the development of computer models and simulations to understand complex natural systems.\n\ncomputational steering\nIs the practice of manually intervening with an otherwise autonomous computational process, to change its outcome.\n\ncomputer\nA device that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called programs. These programs enable computers to perform an extremely wide range of tasks.\n\ncomputer architecture\nA set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation. In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.\n\ncomputer data storage\nAlso simply storage or memory.\nA technology consisting of computer components and recording media that are used to retain digital data. Data storage is a core function and fundamental component of all modern computer systems.\n\ncomputer ethics\nA part of practical philosophy concerned with how computing professionals should make decisions regarding professional and social conduct.\n\ncomputer graphics\nPictures and films created using computers. Usually, the term refers to computer-generated image data created with the help of specialized graphical hardware and software. It is a vast and recently developed area of computer science.\n\ncomputer network\nAlso data network.\nA digital telecommunications network which allows nodes to share resources. In computer networks, computing devices exchange data with each other using connections (data links) between nodes. These data links are established over cable media such as wires or optic cables, or wireless media such as Wi-Fi.\n\ncomputer program\nIs a collection of instructions that can be executed by a computer to perform a specific task. \n\ncomputer programming\nThe process of designing and building an executable computer program for accomplishing a specific computing task. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.\n\ncomputer science\nThe theory, experimentation, and engineering that form the basis for the design and use of computers. It involves the study of algorithms that process, store, and communicate digital information. A computer scientist specializes in the theory of computation and the design of computational systems.\n\ncomputer scientist\nA person who has acquired the knowledge of computer science, the study of the theoretical foundations of information and computation and their application.\n\ncomputer security\nAlso cybersecurity or information technology security (IT security).\nThe protection of computer systems from theft or damage to their hardware, software, or electronic data, as well as from disruption or misdirection of the services they provide.\n\ncomputer vision\nAn interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.\n\ncomputing\nIs any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes study of algorithmic processes and development of both  hardware and software. It has scientific, engineering, mathematical, technological and social aspects. Major computing fields include computer engineering, computer science, cybersecurity, data science, information systems, information technology and software engineering.\n\nconcatenation\nIn formal language theory and computer programming, string concatenation  is the operation of joining character strings end-to-end.  For example, the concatenation of \"snow\" and \"ball\" is \"snowball\". In certain formalisations of concatenation theory, also called string theory, string concatenation is a primitive notion.\n\nConcurrency\nThe ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability property of a program, algorithm, or problem into order-independent or partially-ordered components or units.\n\nconditional\nAlso conditional statement, conditional expression, and conditional construct.\nA feature of a programming language which performs different computations or actions depending on whether a programmer-specified Boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.\n\ncontainer\nIs a class, a data structure, or an abstract data type (ADT) whose instances are collections of other objects. In other words, they store objects in an organized way that follows specific access rules. The size of the container depends on the number of objects (elements) it contains. Underlying (inherited) implementations of various container types may vary in size and complexity, and provide flexibility in choosing the right implementation for any given scenario.\n\ncontinuation-passing style (CPS)\nA style of functional programming in which control is passed explicitly in the form of a continuation. This is contrasted with direct style, which is the usual style of programming. Gerald Jay Sussman and Guy L. Steele, Jr. coined the phrase in AI Memo 349 (1975), which sets out the first version of the Scheme programming language.\n\ncontrol flow\nAlso flow of control.\nThe order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.\n\nCreative Commons (CC)\nAn American non-profit organization devoted to expanding the range of creative works available for others to build upon legally and to share. The organization has released several copyright-licenses, known as Creative Commons licenses, free of charge to the public.\n\ncryptography\nOr cryptology,  is the practice and study of techniques for secure communication in the presence of third parties called adversaries. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.\n\nCSV\nSee comma-separated values.\n\ncyberbullying\nAlso cyberharassment or online bullying.\nA form of bullying or harassment using electronic means.\n\ncyberspace\nWidespread, interconnected digital technology.\n\n\n== D ==\ndaemon\nIn multitasking computer operating systems, a daemon ( or ) is a computer program that runs as a background process, rather than being under the direct control of an interactive user. Traditionally, the process names of a daemon end with the letter d, for clarification that the process is in fact a daemon, and for differentiation between a daemon and a normal computer program. For example, syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.\n\ndata center\nAlso data centre.\nA dedicated space used to house computer systems and associated components, such as telecommunications and data storage systems. It generally includes redundant or backup components and infrastructure for power supply, data communications connections, environmental controls (e.g. air conditioning and fire suppression) and various security devices.\n\ndatabase\nAn organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex, they are often developed using formal design and modeling techniques.\n\ndata mining\nIs a process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use. Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. \n\ndata science\nAn interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. Data science is a \"concept to unify statistics, data analysis, machine learning and their related methods\" in order to \"understand and analyze actual phenomena\" with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.\n\ndata structure\nA data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.\n\ndata type\nAlso simply type.\nAn attribute of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support common data types of real, integer, and Boolean. A data type constrains the values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored. A type of value from which an expression may take its value.\n\ndebugging\nThe process of finding and resolving defects or problems within a computer program that prevent correct operation of computer software or the system as a whole. Debugging tactics can involve interactive debugging, control flow analysis, unit testing, integration testing, log file analysis, monitoring at the application or system level, memory dumps, and profiling.\n\ndeclaration\nIn computer programming, a language construct that specifies properties of an identifier: it declares what a word (identifier) \"means\". Declarations are most commonly used for functions, variables, constants, and classes, but can also be used for other entities such as enumerations and type definitions. Beyond the name (the identifier itself) and the kind of entity (function, variable, etc.), declarations typically specify the data type (for variables and constants), or the type signature (for functions); types may also include dimensions, such as for arrays. A declaration is used to announce the existence of the entity to the compiler; this is important in those strongly typed languages that require functions, variables, and constants, and their types, to be specified with a declaration before use, and is used in forward declaration. The term \"declaration\" is frequently contrasted with the term \"definition\", but meaning and usage varies significantly between languages.\n\ndigital data\nIn information theory and information systems, the discrete, discontinuous representation of information or works. Numbers and letters are commonly used representations.\n\ndigital signal processing (DSP)\nThe use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations.  The signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency.\n\ndiscrete event simulation (DES)\nA model of the operation of a system as a discrete sequence of events in time. Each event occurs at a particular instant in time and marks a change of state in the system. Between consecutive events, no change in the system is assumed to occur; thus the simulation can directly jump in time from one event to the next.\n\ndisk storage\n(Also sometimes called drive storage) is a general category of storage mechanisms where data is recorded by various electronic, magnetic, optical, or mechanical changes to a surface layer of one or more rotating disks. A disk drive is a device implementing such a storage mechanism. Notable types are the hard disk drive (HDD) containing a non-removable disk, the  floppy disk drive (FDD) and its removable floppy disk, and various optical disc drives (ODD) and associated optical disc media.\n\ndistributed computing\nA field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.\n\ndivide and conquer algorithm\nAn algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.\n\nDNS\nSee Domain Name System.\n\ndocumentation\nWritten text or illustration that accompanies computer software or is embedded in the source code. It either explains how it operates or how to use it, and may mean different things to people in different roles.\n\ndomain\nIs the targeted subject area of a computer program. It is a term used in software engineering. Formally it represents the target subject of a specific programming project, whether narrowly or broadly defined.\n\nDomain Name System (DNS)\nA hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or to a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. By providing a worldwide, distributed directory service, the Domain Name System has been an essential component of the functionality of the Internet since 1985.\n\ndouble-precision floating-point format\nA computer number format. It represents a wide dynamic range of numerical values by using a floating radix point.\n\ndownload\nIn computer networks, to receive data from a remote system, typically a server such as a web server, an FTP server, an email server, or other similar systems. This contrasts with uploading, where data is sent to a remote server. A download is a file offered for downloading or that has been downloaded, or the process of receiving such a file.\n\n\n== E ==\nedge device\nA device which provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.\n\nencryption\nIn cryptography, encryption is the process of encoding information. This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext. Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information. Encryption does not itself prevent interference but denies the intelligible content to a would-be interceptor. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, considerable computational resources and skills are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users. Historically, various forms of encryption have been used to aid in cryptography. Early encryption techniques were often utilized in military messaging. Since then, new techniques have emerged and become commonplace in all areas of modern computing. Modern encryption schemes utilize the concepts of public-key and symmetric-key. Modern encryption techniques ensure security because modern computers are inefficient at cracking the encryption.\n\nevent\nAn action or occurrence recognized by software, often originating asynchronously from the external environment, that may be handled by the software. Because an event is an entity which encapsulates the action and the contextual variables triggering the action, the acrostic mnemonic \"Execution Variable Encapsulating Named Trigger\" is often used to clarify the concept.\n\nevent-driven programming\nA programming paradigm in which the flow of the program is determined by events such as user actions (mouse clicks, key presses), sensor outputs, or messages from other programs or threads. Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g. JavaScript web applications) that are centered on performing certain actions in response to user input. This is also true of programming for device drivers (e.g. P in USB device driver stacks).\n\nevolutionary computing\nA family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial-and-error problem-solvers with a metaheuristic or stochastic optimization character.\n\nexecutable\nAlso executable code, executable file, executable program, or simply executable.\nCauses a computer \"to perform indicated tasks according to encoded instructions,\" as opposed to a data file that must be parsed by a program to be meaningful. The exact interpretation depends upon the use - while \"instructions\" is traditionally taken to mean machine code instructions for a physical CPU, in some contexts a file containing bytecode or scripting language instructions may also be considered executable.\n\nexecutable module\n\nexecution\nIn computer and software engineering is the process by which a computer or  virtual machine executes the instructions of a computer program. Each instruction of a program is a description of a particular \naction which to be carried out in order for a specific problem to be solved; as instructions of a program and therefore the actions they describe are being carried out by an executing machine, specific effects are produced in accordance to the semantics of the instructions being executed. \n\nexception handling\nThe process of responding to the occurrence, during computation, of exceptions \u2013 anomalous or exceptional conditions requiring special processing \u2013 often disrupting the normal flow of program execution. It is provided by specialized programming language constructs, computer hardware mechanisms like interrupts, or operating system IPC facilities like signals.\n\nexpression\nIn a programming language, a combination of one or more constants, variables, operators, and functions that the programming language interprets (according to its particular rules of precedence and of association) and computes to produce (\"to return\", in a stateful environment) another value. This process, as for mathematical expressions, is called evaluation.\n\nexternal library\n\n\n== F ==\nfault-tolerant computer system\nA system designed around the concept of fault tolerance. In essence, they must be able to continue working to a level of satisfaction in the presence of errors or breakdowns.\n\nfeasibility study\nAn investigation which aims to objectively and rationally uncover the strengths and weaknesses of an existing business or proposed venture, opportunities and threats present in the natural environment, the resources required to carry through, and ultimately the prospects for success. In its simplest terms, the two criteria to judge feasibility are cost required and value to be attained.\n\nfield\nData that has several parts, known as a record, can be divided into fields. Relational databases arrange data as sets of database records, so called rows. Each record consists of several fields; the fields of all records form the columns.\nExamples of fields: name, gender, hair colour. \n\nfilename extension\nAn identifier specified as a suffix to the name of a computer file. The extension indicates a characteristic of the file contents or its intended use.\n\nfilter (software)\nA computer program or subroutine to process a stream, producing another stream. While a single filter can be used individually, they are frequently strung together to form a pipeline.\n\nfloating point arithmetic\nIn computing, floating-point arithmetic (FP) is arithmetic using formulaic representation of real numbers as an approximation to support a trade-off between range and precision. For this reason, floating-point computation is often found in systems which include very small and very large real numbers, which require fast processing times. A number is, in general, represented approximately to a fixed number of significant digits (the significand) and scaled using an exponent in some fixed base; the base for the scaling is normally two, ten, or sixteen. A number that can be represented exactly is of the following form:\n\n  \n    \n      \n        \n          significand\n        \n        \u00d7\n        \n          \n            base\n          \n          \n            exponent\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\text{significand}}\\times {\\text{base}}^{\\text{exponent}},}\n  \nwhere significand is an integer, base is an integer greater than or equal to two, and exponent is also an integer.\nFor example:\n\n  \n    \n      \n        1.2345\n        =\n        \n          \n            \n              12345\n              \u23df\n            \n          \n          \n            significand\n          \n        \n        \u00d7\n        \n          \n            \n              10\n              \u23df\n            \n          \n          \n            base\n          \n        \n        \n        \n        \n        \n        \n        \n          \n          \n            \n              \n                \n                  \n                    \u2212\n                    4\n                  \n                  \u23de\n                \n              \n              \n                exponent\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle 1.2345=\\underbrace {12345} _{\\text{significand}}\\times \\underbrace {10} _{\\text{base}}\\!\\!\\!\\!\\!\\!^{\\overbrace {-4} ^{\\text{exponent}}}.}\n  for loop\nAlso for-loop. \nA control flow statement for specifying iteration, which allows code to be executed repeatedly. Various keywords are used to specify this statement: descendants of ALGOL use \"for\", while descendants of Fortran use \"do\". There are also other possibilities, e.g. COBOL uses \"PERFORM VARYING\".\n\nformal methods\nA set of mathematically based techniques for the specification, development, and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.\n\nformal verification\nThe act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.\n\nfunctional programming\nA programming paradigm\u2014a style of building the structure and elements of computer programs\u2013that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm in that programming is done with expressions or declarations instead of statements.\n\n\n== G ==\ngame theory\nThe study of mathematical models of strategic interaction between rational decision-makers. It has applications in all fields of social science, as well as in logic and computer science. Originally, it addressed zero-sum games, in which each participant's gains or losses are exactly balanced by those of the other participants. Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.\n\ngarbage in, garbage out (GIGO)\nA term used to describe the concept that flawed or nonsense input data produces nonsense output or \"garbage\".\n\nGraphics Interchange Format\n\ngigabyte\nA multiple of the unit byte for digital information. The prefix giga means 109 in the International System of Units (SI). Therefore, one gigabyte is 1000000000bytes.  The unit symbol for the gigabyte is GB.\n\nglobal variable\nIn computer programming, a variable with global scope, meaning that it is visible (hence accessible) throughout the program, unless shadowed. The set of all global variables is known as the global environment or global state. In compiled languages, global variables are generally static variables, whose extent (lifetime) is the entire runtime of the program, though in interpreted languages (including command-line interpreters), global variables are generally dynamically allocated when declared, since they are not known ahead of time.\n\ngraph theory\nIn mathematics, the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically.\n\n\n== H ==\nhandle\nIn computer programming, a handle is an abstract reference to a resource that is used when application software references blocks of memory or objects that are managed by another system like a database or an operating system.\n\nhard problem\nComputational complexity theory focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\n\nhash function\nAny function that can be used to map data of arbitrary size to data of a fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. Hash functions are often used in combination with a hash table, a common data structure used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file.\n\nhash table\nIn computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.\n\nheap\nA specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: if P is a parent node of C, then the key (the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C. The node at the \"top\" of the heap (with no parents) is called the root node.\n\nheapsort\nA comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.\n\nhuman-computer interaction (HCI)\nResearches the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. As a field of research, human\u2013computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study.\n\n\n== I ==\nidentifier\nIn computer languages, identifiers are tokens (also called symbols) which name language entities. Some of the kinds of entities an identifier might denote include variables, types, labels, subroutines,  and packages.\n\nIDE\nIntegrated development environment.\n\nimage processing\n\nimperative programming\nA programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\n\nincremental build model\nA method of software development where the product is designed, implemented and tested incrementally (a little more is added each time) until the product is finished. It involves both development and maintenance. The product is defined as finished when it satisfies all of its requirements. This model combines the elements of the waterfall model with the iterative philosophy of prototyping.\n\ninformation space analysis\nA deterministic method, enhanced by machine intelligence, for locating and assessing resources for team-centric efforts.\n\ninformation visualization\n\ninheritance\nIn object-oriented programming, the mechanism of basing an object or class upon another object (prototype-based inheritance) or class (class-based inheritance), retaining similar implementation. Also defined as deriving new classes (sub classes) from existing ones (super class or base class) and forming them into a hierarchy of classes.\n\ninput/output (I/O)\nAlso informally io or IO. \nThe communication between an information processing system, such as a computer, and the outside world, possibly a human or another information processing system. Inputs are the signals or data received by the system and outputs are the signals or data sent from it. The term can also be used as part of an action; to \"perform I/O\" is to perform an input or output operation.\n\ninsertion sort\nA simple sorting algorithm that builds the final sorted array (or list) one item at a time.\n\ninstruction cycle\nAlso fetch\u2013decode\u2013execute cycle or simply fetch-execute cycle.\nThe cycle which the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. It is composed of three main stages: the fetch stage, the decode stage, and the execute stage.\n\ninteger\nA datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware, including virtual machines, nearly always provide a way to represent a processor register or memory address as an integer.\n\nintegrated development environment (IDE)\nA software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools, and a debugger.\n\nintegration testing\n(sometimes called integration and testing, abbreviated I&T) is the phase in software testing in which individual software modules are combined and tested as a group. Integration testing is conducted to evaluate the compliance of a system or component with specified functional requirements. It occurs after unit testing and before validation testing. Integration testing takes as its input modules that have been unit tested, groups them in larger aggregates, applies tests defined in an integration test plan to those aggregates, and delivers as its output the integrated system ready for system testing.\n\nintellectual property (IP)\nA category of legal property that includes intangible creations of the human intellect. There are many types of intellectual property, and some countries recognize more than others. The most well-known types are copyrights, patents, trademarks, and trade secrets.\n\nintelligent agent\nIn artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex. A reflex machine, such as a thermostat, is considered an example of an intelligent agent.\n\ninterface\nA shared boundary across which two or more separate components of a computer system exchange information. The exchange can be between software, computer hardware, peripheral devices, humans, and combinations of these. Some computer hardware devices, such as a touchscreen, can both send and receive data through the interface, while others such as a mouse or microphone may only provide an interface to send data to a given system.\n\ninternal documentation\nComputer software is said to have Internal Documentation if the notes on how and why various parts of code operate is included within the source code as comments.  It is often combined with meaningful variable names with the intention of providing potential future programmers a means of understanding the workings of the code. This contrasts with external documentation, where programmers keep their notes and explanations in a separate document.\n\ninternet\nThe global system of interconnected computer networks that use the Internet protocol suite (TCP/IP) to link devices worldwide. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies.\n\ninternet bot\nAlso web robot, robot, or simply bot.\nA software application that runs automated tasks (scripts) over the Internet. Typically, bots perform tasks that are both simple and structurally repetitive, at a much higher rate than would be possible for a human alone. The largest use of bots is in web spidering (web crawler), in which an automated script fetches, analyzes and files information from web servers at many times the speed of a human.\n\ninterpreter\nA computer program that directly executes instructions written in a programming or scripting language, without requiring them to have been previously compiled into a machine language program.\n\ninvariant\nOne can encounter invariants that can be relied upon to be true during the execution of a program, or during some portion of it. It is a logical assertion that is always held to be true during a certain phase of execution. For example, a loop invariant is a condition that is true at the beginning and the end of every execution of a loop.\n\niteration\nIs the repetition of a process in order to generate an outcome. The sequence will approach some end point or end value. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration.  In mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms.\n\n\n== J ==\nJava\nA general-purpose programming language that is class-based, object-oriented(although not a pure OO language), and designed to have as few implementation dependencies as possible. It is intended to let application developers \"write once, run anywhere\" (WORA), meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.\n\n\n== K ==\nkernel\nThe first section of an operating system to load into memory. As the center of the operating system, the kernel needs to be small, efficient, and loaded into a protected area in the memory so that it cannot be overwritten. It may be responsible for such essential tasks as disk drive management, file management, memory management, process management, etc.\n\n\n== L ==\nlibrary (computing)\nA collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values, or type specifications.\n\nlinear search\nAlso sequential search.\nA method for finding an element within a list. It sequentially checks each element of the list until a match is found or the whole list has been searched.\n\nlinked list\nA linear collection of data elements, whose order is not given by their physical placement in memory. Instead, each element points to the next. It is a data structure consisting of a collection of nodes which together represent a sequence.\n\nlinker\n or link editor, is a computer utility program that takes one or more object files generated by a compiler or an assembler and combines them into a single executable file, library file, or another 'object' file.  A simpler version that writes its output directly to memory is called the loader, though loading is typically considered a separate process.\n\nlist\nAn abstract data type that represents a countable number of ordered values, where the same value may occur more than once. An instance of a list is a computer representation of the mathematical concept of a finite sequence; the (potentially) infinite analog of a list is a stream. Lists are a basic example of containers, as they contain other values. If the same value occurs multiple times, each occurrence is considered a distinct item.\n\nloader\nThe part of an operating system that is responsible for loading programs and libraries. It is one of the essential stages in the process of starting a program, as it places programs into memory and prepares them for execution. Loading a program involves reading the contents of the executable file containing the program instructions into memory, and then carrying out other required preparatory tasks to prepare the executable for running. Once loading is complete, the operating system starts the program by passing control to the loaded program code.\n\nlogic error\nIn computer programming, a bug in a program that causes it to operate incorrectly, but not to terminate abnormally (or crash). A logic error produces unintended or undesired output or other behaviour, although it may not immediately be recognized as such.\n\nlogic programming\nA type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, answer set programming (ASP), and Datalog.\n\n\n== M ==\nmachine learning (ML)\nThe scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n\nmachine vision (MV)\nThe technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry. Machine vision refers to many technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science. It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.\n\nmathematical logic\nA subfield of mathematics exploring the applications of formal logic to mathematics.  It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science. The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.\n\nmatrix\nIn mathematics, a matrix, (plural matrices), is a rectangular array (see irregular matrix) of numbers, symbols, or expressions, arranged in rows and columns.\n\nmemory\nComputer data storage, often called storage, is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.\n\nmerge sort\nAlso mergesort.\nAn efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and von Neumann as early as 1948.\n\nmethod\nIn object-oriented programming (OOP), a procedure associated with a message and an object. An object consists of data and behavior. The data and behavior comprise an interface, which specifies how the object may be utilized by any of various consumers of the object.\n\nmethodology\nIn software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management. It is also known as a software development life cycle (SDLC). The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.\n\nmodem\nPortmanteau of modulator-demodulator.\nA hardware device that converts data into a format suitable for a transmission medium so that it can be transmitted from one computer to another (historically along telephone wires). A modem modulates one or more carrier wave signals to encode digital information for transmission and demodulates signals to decode the transmitted information. The goal is to produce a signal that can be transmitted easily and decoded reliably to reproduce the original digital data. Modems can be used with almost any means of transmitting analog signals from light-emitting diodes to radio. A common type of modem is one that turns the digital data of a computer into modulated electrical signal for transmission over telephone lines and demodulated by another modem at the receiver side to recover the digital data.\n\n\n== N ==\nnatural language processing (NLP)\nA subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.  Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n\nnode\nIs a basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers.\n\nnumber theory\nA branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions.\n\nnumerical analysis\nThe study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).\n\nnumerical method\nIn numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems. The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm.\n\n\n== O ==\nobject\nAn object can be a variable, a data structure, a function, or a method, and as such, is a value in memory referenced by an identifier.  In the class-based object-oriented programming paradigm, object refers to a particular instance of a class, where the object can be a combination of variables, functions, and data structures.  In relational database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).\n\nobject code\nAlso object module.\nThe product of a compiler. In a general sense object code is a sequence of statements or instructions in a computer language, usually a machine code language (i.e., binary) or an intermediate language such as register transfer language (RTL). The term indicates that the code is the goal or result of the compiling process, with some early sources referring to source code as a \"subject program.\"\n\nobject-oriented analysis and design (OOAD)\nA technical approach for analyzing and designing an application, system, or business by applying object-oriented programming, as well as using visual modeling throughout the software development process to guide stakeholder communication and product quality.\n\nobject-oriented programming (OOP)\nA programming paradigm based on the concept of \"objects\", which can contain data, in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods). A feature of objects is an object's procedures that can access and often modify the data fields of the object with which they are associated (objects have a notion of \"this\" or \"self\"). In OOP, computer programs are designed by making them out of objects that interact with one another. OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.\n\nopen-source software (OSS)\nA type of computer software in which source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. Open-source software is a prominent example of open collaboration.\n\noperating system (OS)\nSystem software that manages computer hardware, software resources, and provides common services for computer programs.\n\noptical fiber\nA flexible, transparent fiber made by drawing glass (silica) or plastic to a diameter slightly thicker than that of a human hair. Optical fibers are used most often as a means to transmit light between the two ends of the fiber and find wide usage in fiber-optic communications, where they permit transmission over longer distances and at higher bandwidths (data rates) than electrical cables. Fibers are used instead of metal wires because signals travel along them with less loss; in addition, fibers are immune to electromagnetic interference, a problem from which metal wires suffer.\n\n\n== P ==\npair programming\nAn agile software development technique in which two programmers work together at one workstation. One, the driver, writes code while the other, the observer or navigator, reviews each line of code as it is typed in. The two programmers switch roles frequently.\n\nparallel computing\nA type of computation in which many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism.\n\nparameter\nAlso formal argument.\nIn computer programming, a special kind of variable, used in a subroutine to refer to one of the pieces of data provided as input to the subroutine. These pieces of data are the values of the arguments (often called actual arguments or actual parameters) with which the subroutine is going to be called/invoked. An ordered list of parameters is usually included in the definition of a subroutine, so that, each time the subroutine is called, its arguments for that call are evaluated, and the resulting values can be assigned to the corresponding parameters.\n\nperipheral\nAny auxiliary or ancillary device connected to or integrated within a computer system and used to send information to or retrieve information from the computer. An input device sends data or instructions to the computer; an output device provides output from the computer to the user; and an input/output device performs both functions.\n\npointer\nIs an object in many programming languages that stores a memory address. This can be that of another value located in computer memory, or in some cases, that of memory-mapped computer hardware. A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer. As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on that page. The actual format and content of a pointer variable is dependent on the underlying computer architecture.\n\npostcondition\nIn computer programming, a condition or predicate that must always be true just after the execution of some section of code or after an operation in a formal specification. Postconditions are sometimes tested using assertions within the code itself. Often, postconditions are simply included in the documentation of the affected section of code.\n\nprecondition\nIn computer programming, a condition or predicate that must always be true just prior to the execution of some section of code or before an operation in a formal specification.  If a precondition is violated, the effect of the section of code becomes undefined and thus may or may not carry out its intended work.  Security problems can arise due to incorrect preconditions.\n\nprimary storage\n(Also known as main memory, internal memory or prime memory), often referred to simply as memory, is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in uniform manner.\n\nprimitive data type\n\npriority queue\nAn abstract data type which is like a regular queue or stack data structure, but where additionally each element has a \"priority\" associated with it. In a priority queue, an element with high priority is served before an element with low priority. In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued, while in other implementations, ordering of elements with the same priority is undefined.\n\nprocedural programming\n\nprocedure\nIn computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.  Subroutines may be defined within programs, or separately in libraries that can be used by many programs.  In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure. Technically, these terms all have different definitions. The generic, umbrella term  callable unit is sometimes used.\n\nprogram lifecycle phase\nProgram lifecycle phases are the stages a computer program undergoes, from initial creation to deployment and execution. The phases are edit time, compile time, link time, distribution time, installation time, load time, and run time.\n\nprogramming language\nA formal language, which comprises a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms.\n\nprogramming language implementation\nIs a system for executing computer programs. There are two general approaches to programming language implementation: interpretation and compilation.\n\nprogramming language theory\n(PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and of their individual features.  It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science.  It has become a well-recognized branch of computer science, and an active research area, with results published in numerous  journals dedicated to PLT, as well as in general computer science and engineering publications.\n\nProlog\nIs a logic programming language associated with artificial intelligence and computational linguistics.  Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules.  A computation is initiated by running a query over these relations.\n\nPython\nIs an interpreted, high-level and general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n\n\n== Q ==\nquantum computing\nThe use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. A quantum computer is used to perform such computation, which can be implemented theoretically or physically.\n\nqueue\nA collection in which the entities in the collection are kept in order and the principal (or only) operations on the collection are the addition of entities to the rear terminal position, known as enqueue, and removal of entities from the front terminal position, known as dequeue.\n\nquicksort\nAlso partition-exchange sort.\nAn efficient sorting algorithm which serves as a systematic method for placing the elements of a random access file or an array in order.\n\n\n== R ==\nR programming language\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\nradix\nAlso base.\nIn digital numeral systems, the number of unique digits, including the digit zero, used to represent numbers in a positional numeral system. For example, in the decimal/denary system (the most common system in use today) the radix (base number) is ten, because it uses the ten digits from 0 through 9, and all other numbers are uniquely specified by positional combinations of these ten base digits; in the binary system that is the standard in computing, the radix is two, because it uses only two digits, 0 and 1, to uniquely specify each number.\n\nrecord\nA record (also called a structure,  struct, or compound data) is a basic data structure. Records in a database or spreadsheet are usually called \"rows\".\n\nrecursion\nOccurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no infinite loop or infinite chain of references can occur.\n\nreference\nIs a value that enables a program to indirectly access a particular datum, such as a variable's value or a record, in the computer's memory or in some other storage device.  The reference is said to refer to the datum, and accessing the datum is called dereferencing the reference.\n\nreference counting\nA programming technique of storing the number of references, pointers, or handles to a resource, such as an object, a block of memory, disk space, and others. In garbage collection algorithms, reference counts may be used to deallocate objects which are no longer needed.\n\nrelational database\nIs a digital database based on the relational model of data, as proposed by E. F. Codd in 1970.\nA software system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems have an option of using the SQL (Structured Query Language) for querying and maintaining the database.\n\nreliability engineering\nA sub-discipline of systems engineering that emphasizes dependability in the lifecycle management of a product. Reliability describes the ability of a system or component to function under stated conditions for a specified period of time. Reliability is closely related to availability, which is typically described as the ability of a component or system to function at a specified moment or interval of time.\n\nregression testing\n(rarely non-regression testing) is re-running functional and non-functional tests to ensure that previously developed and tested software still performs after a change. If not, that would be called a regression. Changes that may require regression testing include bug fixes, software enhancements, configuration changes, and even substitution of electronic components. As regression test suites tend to grow with each found defect, test automation is frequently involved. Sometimes a change impact analysis is performed to determine an appropriate subset of tests (non-regression analysis).\n\nrequirements analysis\nIn systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.\n\nrobotics\nAn interdisciplinary branch of engineering and science that includes mechanical engineering, electronic engineering, information engineering, computer science, and others. Robotics involves design, construction, operation, and use of robots, as well as computer systems for their perception, control, sensory feedback, and information processing. The goal of robotics is to design intelligent machines that can help and assist humans in their day-to-day lives and keep everyone safe.\n\nround-off error\nAlso rounding error.\nThe difference between the result produced by a given algorithm using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic. Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them. This is a form of quantization error. When using approximation equations or algorithms, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits), one of the goals of numerical analysis is to estimate computation errors. Computation errors, also called numerical errors, include both truncation errors and roundoff errors.\n\nrouter\nA networking device that forwards data packets between computer networks. Routers perform the traffic directing functions on the Internet.  Data sent through the internet, such as a web page or email, is in the form of data packets.   A packet is typically forwarded from one router to another router through the networks that constitute an internetwork (e.g. the Internet) until it reaches its destination node.\n\nrouting table\nIn computer networking a routing table, or routing information base (RIB), is a data table stored in a router or a network host that lists the routes to particular network destinations, and in some cases, metrics (distances) associated with those routes. The routing table contains information about the topology of the network immediately around it.\n\nrun time\nRuntime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code. In other words, \"runtime\" is the running phase of a program.\n\nrun time error\nA runtime error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at runtime depending on the particular language and compiler. Many other runtime errors exist and are handled differently by different programming languages, such as division by zero errors, domain errors, array subscript out of bounds errors, arithmetic underflow errors, several types of underflow and overflow errors, and many other runtime errors generally considered as software bugs which may or may not be caught and handled by any particular computer language. \n\n\n== S ==\nsearch algorithm\nAny algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values.\n\nsecondary storage\nAlso known as external memory or auxiliary storage, differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile (retaining data when power is shut off). Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive.\n\nselection sort\nIs an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.\n\nsemantics\nIn programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.\n\nsequence\nIn mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order does matter.  Like a set, it contains members (also called elements, or terms).  The number of elements (possibly infinite) is called the length of the sequence.  Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order does matter.  Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first n natural numbers (for a sequence of finite length n).\n\nThe position of an element in a sequence is its rank or index; it is the natural number for which the element is the image. The first element has index 0 or 1, depending on the context or a specific convention.  When a symbol is used to denote a sequence, the nth element of the sequence is denoted by this symbol with n as subscript; for example, the nth element of the Fibonacci sequence F is generally denoted Fn.\n\nFor example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last.  This sequence differs from (A, R, M, Y).  Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence.  Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...).  In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams.  The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.\n\nserializability\nIn concurrency control of databases, transaction processing (transaction management), and various transactional applications (e.g., transactional memory and software transactional memory), both centralized and distributed, a transaction schedule is serializable if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e. without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions. It is considered the highest level of isolation between transactions, and plays an essential role in concurrency control. As such it is supported in all general purpose database systems. Strong strict two-phase locking (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.\n\nserialization\nIs the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment). When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. For many complex objects, such as those that make extensive use of references, this process is not straightforward. Serialization of object-oriented objects does not include any of their associated methods with which they were previously linked.\n\nThis process of serializing an object is also called marshalling an object in some situations.[3][4] The opposite operation, extracting a data structure from a series of bytes, is deserialization, (also called unserialization or unmarshalling).\n\nservice level agreement\n(SLA), is a commitment between a service provider and a client. Particular aspects of the service \u2013 quality, availability, responsibilities \u2013 are agreed between the service provider and the service user. The most common component of an SLA is that the services should be provided to the customer as agreed upon in the contract. As an example, Internet service providers and telcos will commonly include service level agreements within the terms of their contracts with customers to define the level(s) of service being sold in plain language terms. In this case the SLA will typically have a technical definition in  mean time between failures (MTBF), mean time to repair or mean time to recovery (MTTR); identifying which party is responsible for reporting faults or paying fees; responsibility for various data rates; throughput; jitter; or similar measurable details.\n\nset\nIs an abstract data type that can store unique values, without any particular order. It is a computer implementation of the mathematical concept of a finite set. Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set.\n\nsoft computing\n\nsoftware\nComputer software, or simply software, is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.\n\nsoftware agent\nIs a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. Such \"action on behalf of\" implies the authority to decide which, if any, action is appropriate. Agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or  as software such as a chatbot\nexecuting on a phone (e.g. Siri)  or other computing device.  Software agents may be autonomous or work together with other agents or people.  Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).\n\nsoftware construction\nIs a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.\n\nsoftware deployment\nIs all of the activities that make a software system available for use.\n\nsoftware design\nIs the process by which an agent creates a specification of a software artifact, intended to accomplish goals, using a set of primitive components and subject to constraints. Software design may refer to either \"all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems\" or \"the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.\"\n\nsoftware development\nIs the process of conceiving, specifying, designing, programming, documenting, testing, and bug fixing involved in creating and maintaining applications, frameworks, or other software components. Software development is a process of writing and maintaining the source code, but in a broader sense, it includes all that is involved between the conception of the desired software through to the final manifestation of the software, sometimes in a planned and structured process. Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.\n\nsoftware development process\nIn software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.  It is also known as a software development life cycle (SDLC).  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.  Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.\n\nsoftware engineering\nIs the systematic application of engineering approaches to the development of software. Software engineering is a computing discipline.\n\nsoftware maintenance\nIn software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.\n\nsoftware prototyping\nIs the activity of creating prototypes of software applications, i.e., incomplete versions of the software program being developed. It is an activity that can occur in software development and is comparable to prototyping as known from other fields, such as mechanical engineering or manufacturing.  A prototype typically simulates only a few aspects of, and may be completely different from, the final product. \n\nsoftware requirements specification\n(SRS), is a description of a software system to be  developed. The software requirements specification lays out functional and non-functional requirements, and it may include a set of use cases that describe user interactions that the software must provide to the user for perfect interaction.\n\nsoftware testing\nIs an investigation conducted to provide stakeholders with information about the quality of the software product or service under test. Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation. Test techniques include the process of executing a program or application with the intent of finding software bugs (errors or other defects), and verifying that the software product is fit for use. \n\nsorting algorithm\nIs an algorithm that puts elements of a list in a certain order. The most frequently used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists. Sorting is also often useful for canonicalizing data and for producing human-readable output. More formally, the output of any sorting algorithm must satisfy two conditions:\n\nThe output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);\nThe output is a permutation (a reordering, yet retaining all of the original elements) of the input.\n\nFurther, the input data is often stored in an array, which allows random access, rather than a list, which only allows sequential access; though many algorithms can be applied to either type of data after suitable modification.\n\nsource code\nIn computing, source code is any collection of code, with or without comments, written using a human-readable programming language, usually as plain text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code that can be executed by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.\n\nspiral model\nIs a risk-driven software development process model. Based on the unique risk patterns of a given project, the spiral model guides a team to adopt elements of one or more process models, such as incremental, waterfall, or evolutionary prototyping.\n\nstack\nIs an abstract data type that serves as a collection of elements, with two main principal operations:\npush, which adds an element to the collection, and\npop, which removes the most recently added element that was not yet removed.\nThe order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack. The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other. This structure makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.\n\nstate\nIn information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.\n\nstatement\nIn computer programming, a statement is a syntactic unit of an imperative programming language that expresses some action to be carried out. A program written in such a language is formed by a sequence of one or more statements. A statement may have internal components (e.g., expressions).\n\nstorage\nComputer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.\n\nstream\nIs a sequence of data elements made available over time. A stream can be thought of as items on a conveyor belt being processed one at a time rather than in large batches.\n\nstring\nIn computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures.\n\nstructured storage\nA NoSQL (originally referring to \"non-SQL\" or \"non-relational\") database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name \"NoSQL\" was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications.  NoSQL systems are also sometimes called \"Not only SQL\" to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.\n\nsubroutine\nIn computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.  Subroutines may be defined within programs, or separately in libraries that can be used by many programs.  In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure. Technically, these terms all have different definitions. The generic, umbrella term  callable unit is sometimes used.\n\nsymbolic computation\nIn mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.\n\nsyntax\nThe syntax of a computer language is the set of rules that defines the combinations of symbols that are considered to be correctly structured statements or expressions in that language. This applies both to programming languages, where the document represents source code, and to markup languages, where the document represents data.\n\nsyntax error\nIs an error in the syntax of a sequence of characters or tokens that is intended to be written in compile-time. A program will not compile until all syntax errors are corrected. For interpreted languages, however, a syntax error may be detected during program execution, and an interpreter's error messages might not differentiate syntax errors from errors of other kinds. There is some disagreement as to just what errors are \"syntax errors\". For example, some would say that the use of an uninitialized variable's value in Java code is a syntax error, but many others would disagree and would classify this as a (static) semantic error.\n\nsystem console\nThe system console, computer console, root console, operator's console, or simply console is the text entry and display device for system administration messages, particularly those from the BIOS or boot loader, the kernel, from the init system and from the system logger. It is a physical device consisting of a keyboard and a screen, and traditionally is a text terminal, but may also be a graphical terminal. System consoles are generalized to computer terminals, which are abstracted respectively by virtual consoles and terminal emulators. Today communication with system consoles is generally done abstractly, via the standard streams (stdin, stdout, and stderr), but there may be system-specific interfaces, for example those used by the system kernel.\n\n\n== T ==\ntechnical documentation\nIn engineering, any type of documentation that describes handling, functionality, and architecture of a technical product or a product under development or use. The intended recipient for product technical documentation is both the (proficient) end user as well as the administrator/service or maintenance technician. In contrast to a mere \"cookbook\" manual, technical documentation aims at providing enough information for a user to understand inner and outer dependencies of the product at hand.\n\nthird-generation programming language\nA third-generation programming language (3GL) is a high-level computer programming language that tends to be more machine-independent and programmer-friendly than the machine code of the first-generation and assembly languages of the second-generation, while having a less specific focus to the fourth and fifth generations. Examples of common and historical third-generation programming languages are ALGOL, BASIC, C, COBOL, Fortran, Java, and Pascal.\n\ntop-down and bottom-up design\n\ntree\nA widely used abstract data type (ADT) that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.\n\ntype theory\nIn mathematics, logic, and computer science, a type theory is any of a class of formal systems, some of which can serve as alternatives to set theory as a foundation for all mathematics. In type theory, every \"term\" has a \"type\" and operations are restricted to terms of a certain type.\n\n\n== U ==\nupload\nIn computer networks, to send data to a remote system such as a server or another client so that the remote system can store a copy. Contrast download.\n\nUniform Resource Locator (URL)\nColloquially web address.\nA reference to a web resource that specifies its location on a computer network and a mechanism for retrieving it. A URL is a specific type of Uniform Resource Identifier (URI), although many people use the two terms interchangeably. URLs occur most commonly to reference web pages (http), but are also used for file transfer (ftp), email (mailto), database access (JDBC), and many other applications.\n\nuser\nIs a person who utilizes a computer or network service. Users of computer systems and software products generally lack the technical expertise required to fully understand how they work. Power users use advanced features of programs, though they are not necessarily capable of computer programming and system administration.\n\nuser agent\nSoftware (a software agent) that acts on behalf of a user, such as a web browser that \"retrieves, renders and facilitates end user interaction with Web content\". An email reader is a mail user agent.\n\nuser interface (UI)\nThe space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls. The design considerations applicable when creating user interfaces are related to or involve such disciplines as ergonomics and psychology.\n\nuser interface design\nAlso user interface engineering.\nThe design of user interfaces for machines and software, such as computers, home appliances, mobile devices, and other electronic devices, with the focus on maximizing usability and the user experience. The goal of user interface design is to make the user's interaction as simple and efficient as possible, in terms of accomplishing user goals (user-centered design).\n\n\n== V ==\nvariable\nIn computer programming, a variable, or scalar, is a storage location (identified by a memory address) paired with an associated symbolic name (an identifier), which contains some known or unknown quantity of information referred to as a value. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may therefore change during the course of program execution.\n\nvirtual machine (VM)\nAn emulation of a computer system. Virtual machines are based on computer architectures and attempt to provide the same functionality as a physical computer. Their implementations may involve specialized hardware, software, or a combination of both.\n\nV-Model\nA software development process that may be considered an extension of the waterfall model, and is an example of the more general V-model. Instead of moving down in a linear way, the process steps are bent upwards after the coding phase, to form the typical V shape. The V-Model demonstrates the relationships between each phase of the development life cycle and its associated phase of testing. The horizontal and vertical axes represent time or project completeness (left-to-right) and level of abstraction (coarsest-grain abstraction uppermost), respectively.\n\n\n== W ==\nwaterfall model\nA breakdown of project activities into linear sequential phases, where each phase depends on the deliverables of the previous one and corresponds to a specialisation of tasks.  The approach is typical for certain areas of engineering design. In software development, it tends to be among the less iterative and flexible approaches, as progress flows in largely one direction (\"downwards\" like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, deployment and maintenance.\n\nWaveform Audio File Format\nAlso WAVE or WAV due to its filename extension.\nAn audio file format standard, developed by Microsoft and IBM, for storing an audio bitstream on PCs. It is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in \"chunks\", and thus is also close to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively. It is the main format used on Microsoft Windows systems for raw and typically uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.\n\nweb crawler\nAlso spider, spiderbot, or simply crawler.\nAn Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering).\n\nWi-Fi\nA family of wireless networking technologies, based on the IEEE 802.11 family of standards, which are commonly used for local area networking of devices and Internet access. Wi\u2011Fi is a trademark of the non-profit Wi-Fi Alliance, which restricts the use of the term Wi-Fi Certified to products that successfully complete interoperability certification testing.\n\n\n== X ==\nXHTML\nAbbreviaton of eXtensible HyperText Markup Language.\nPart of the family of XML markup languages. It mirrors or extends versions of the widely used HyperText Markup Language (HTML), the language in which web pages are formulated.\n\n\n== See also ==\nOutline of computer science\n\n\n== References ==\n\n\n== Notes =="
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Category:Computer_science
Category:Computer science - Wikipedia
"
This text should be understood on average by 6 year olds.
https://en.wikipedia.org/wiki/Template:TopicTOC-Computer_science
Template:TopicTOC-Computer science - Wikipedia
"
This text should be understood on average by 6 year olds.
https://en.wikipedia.org/wiki/Template_talk:TopicTOC-Computer_science
Template talk:TopicTOC-Computer science - Wikipedia
"
This text should be understood on average by 6 year olds.
https://en.wikipedia.org/w/index.php?title=Template:TopicTOC-Computer_science&amp;action=edit
Template:TopicTOC-Computer science - Wikipedia
"
This text should be understood on average by 6 year olds.
https://en.wikipedia.org/wiki/Algorithm
Algorithm - Wikipedia
In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of problems or to perform a computation. Algorithms are always unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks.\nAs an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in 240 BC in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.The word algorithm itself is derived from the name of the 9th-century mathematician Mu\u1e25ammad ibn M\u016bs\u0101 al-Khw\u0101rizm\u012b, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi. A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the G\u00f6del\u2013Herbrand\u2013Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936\u201337 and 1939.\n\n\n== Etymology ==\nThe word 'algorithm' has its roots in Latinizing the nisba, indicating his geographic origin, of the name of Persian mathematician Muhammad ibn Musa al-Khwarizmi to algorismus. Al-Khw\u0101rizm\u012b (Arabized Persian \u0627\u0644\u062e\u0648\u0627\u0631\u0632\u0645\u06cc c. 780\u2013850) was a mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan.About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu\u2013Arabic numeral system, which was translated into Latin during the 12th century. The manuscript starts with the phrase Dixit Algorizmi ('Thus spake Al-Khwarizmi'), where \"Algorizmi\" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the \"decimal number system\". In the 15th century, under the influence of the Greek word \u1f00\u03c1\u03b9\u03b8\u03bc\u03cc\u03c2 (arithmos), 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that \"algorithm\" took on the meaning that it has in modern English.Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:\n\nHaec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.\nwhich translates to:\n\nAlgorism is the art by which at present we use those Indian figures, which number two times five.\nThe poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.\n\n\n== Informal definition ==\n\nAn informal definition could be \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure\nor cook-book recipe.In general, a program is only an algorithm if it stops eventually - even though  infinite loops may sometimes prove desirable.\nA prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\nBoolos, Jeffrey & 1974, 1999 offer an informal meaning of the word \"algorithm\" in the following quotation:\n\nNo human being can write fast enough, or long enough, or small enough\u2020 ( \u2020\"smaller and smaller without limit \u2026 you'd be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.\nAn  \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an arbitrary \"input\" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary \"input variables\" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\n\nPrecise instructions (in a language understood by \"the computer\") for a fast, efficient, \"good\" process that specifies the \"moves\" of \"the computer\" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = \u2026 and \"effectively\" produce, in a \"reasonable\" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of  decidability\u2014a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.\n\n\n== Formalization ==\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform\u2014in a specific order\u2014to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):\n\n Minsky: \"But we will also maintain, with Turing \u2026 that any procedure which could \"naturally\" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments \u2026 in its favor are hard to refute\".\n Gurevich: \u201c\u2026 Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine \u2026 according to Savage [1987], an algorithm is a computational process defined by a Turing machine\".\nTuring machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case\u2014due to a major theorem of computability theory known as the halting problem.\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\nFor some of these computational processes, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\"\u2014an idea that is described more formally by flow of control.\nSo far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception\u2014one which attempts to describe a task in discrete, \"mechanical\" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. An example of such an assignment can be found below.\nFor some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.\n\n\n=== Expressing algorithms ===\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see Turing machine for more).\nRepresentations of algorithms can be classed into three accepted levels of Turing machine description, as follows:\n1 High-level description\n\u201c\u2026prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head.\"\n2 Implementation description\n\u201c\u2026prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function.\"\n3 Formal description\nMost detailed, \"lowest level\", gives the Turing machine's \"state table\".For an example of the simple algorithm \"Add m+n\" described in all three levels, see Algorithm#Examples.\n\n\n== Design ==\n\nAlgorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.\nOne of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size its input increases.\nTypical steps in the development of algorithms:\n\nProblem definition\nDevelopment of a model\nSpecification of the algorithm\nDesigning an algorithm\nChecking the correctness of the algorithm\nAnalysis of algorithm\nImplementation of algorithm\nProgram testing\nDocumentation preparation\n\n\n== Implementation ==\n\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n\n\n== Computer algorithms ==\n\nIn computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended \"target\" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.\n\"Elegant\" (compact) programs, \"good\" (fast) programs : The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\n\nKnuth: \" \u2026 we want good algorithms in some loosely defined aesthetic sense. One criterion \u2026 is the length of time taken to perform the algorithm \u2026. Other criteria are adaptability of the algorithm to computers, its simplicity and elegance, etc\"Chaitin: \" \u2026 a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does\"Chaitin prefaces his definition with: \"I'll show you can't prove that a program is 'elegant'\"\u2014such a proof would solve the Halting problem (ibid).\nAlgorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms\".Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)\u2014an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.\nComputers (and computors), models of computation: A computer (or human \"computor\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek's \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF\u2013THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L \u2190 0), SUCCESSOR (e.g. L \u2190 L+1), and DECREMENT (e.g. L \u2190 L \u2212 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.  However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction \" Z \u2190 0 \"; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.\nSimulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky's \"decrement\").\nStructured programming, canonical structures: Per the Church\u2013Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types\u2014conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three B\u00f6hm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The B\u00f6hm\u2013Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.\n\n\n== Examples ==\n\n\n=== Algorithm example ===\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:\nHigh-level description:\n\nIf there are no numbers in the set then there is no highest number.\nAssume the first number in the set is the largest number in the set.\nFor each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\nWhen there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\n\n=== Euclid's algorithm ===\n\nEuclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII (\"Elementary Number Theory\") of his Elements. Euclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l \u2212 q\u00d7s, q being the quotient, or remainder r is the \"modulus\", the integer-fractional part left over after the division.For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \u201cproper\u201d; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).\nEuclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest. While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.\n\n\n==== Computer language for Euclid's algorithm ====\nOnly a few instruction types are required to execute Euclid's algorithm\u2014some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\n\nA location is symbolized by upper case letter(s), e.g. S, A, etc.\nThe varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.\n\n\n==== An inelegant program for Euclid's algorithm ====\n\nThe following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2\u20134:\nINPUT:\n\n1 [Into two locations L and S put the numbers l and s that represent the two lengths]:\n  INPUT L, S\n2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:\n  R \u2190 L\n\nE0: [Ensure r \u2265 s.]\n\n3 [Ensure the smaller of the two numbers is in S and the larger in R]:\n  IF R > S THEN\n    the contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:\n    GOTO step 6\n  ELSE\n    swap the contents of R and S.\n4   L \u2190 R (this first step is redundant, but is useful for later discussion).\n5   R \u2190 S\n6   S \u2190 L\n\nE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.\n\n7 IF S > R THEN\n    done measuring so\n    GOTO 10\n  ELSE\n    measure again,\n8   R \u2190 R \u2212 S\n9   [Remainder-loop]:\n    GOTO 7.\n\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\n\n10 IF R = 0 THEN\n     done so\n     GOTO step 15\n   ELSE\n     CONTINUE TO step 11,\n\nE3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.\n\n11  L \u2190 R\n12  R \u2190 S\n13  S \u2190 L\n14  [Repeat the measuring process]:\n    GOTO 7\n\nOUTPUT:\n\n15 [Done. S contains the greatest common divisor]:\n   PRINT S\n\nDONE:\n\n16 HALT, END, STOP.\n\n\n==== An elegant program for Euclid's algorithm ====\nThe following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more types of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by \u2190.\n\nHow \"Elegant\" works: In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A \u2190 A \u2212 B, and a B \u2264 A loop that computes B \u2190 B \u2212 A. This works because, when at last the minuend M is less than or equal to the subtrahend S (Difference = Minuend \u2212 Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the \"sense\" of the subtraction reverses.\nThe following version can be used with programming languages from the C-family:\n\n\n=== Testing the Euclid algorithms ===\nDoes an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\nBut \"exceptional cases\" must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\nProof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid's algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\n\n\n=== Measuring and improving the Euclid algorithms ===\nElegance (compactness) versus goodness (speed): With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does two conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\nCan the algorithms be improved?: Once the programmer judges a program \"fit\" and \"effective\"\u2014that is, it computes the function intended by its author\u2014then the question becomes, can it be improved?\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\n\n\n== Algorithmic analysis ==\n\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.\n\n\n=== Formal versus empirical ===\n\nThe analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\n\n\n=== Execution efficiency ===\n\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\n\n== Classification ==\nThere are various ways to classify algorithms, each with its own merits.\n\n\n=== By implementation ===\nOne way to classify algorithms is by implementation means.\n\nRecursion\nA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\nLogical\nAn algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic programming paradigm. In pure logic programming languages, the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant semantics: a change in the axioms produces a well-defined change in the algorithm.\nSerial, parallel or distributed\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines connected with a computer network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no parallel algorithms and are called inherently serial problems.\nDeterministic or non-deterministic\nDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\nExact or approximate\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.\nQuantum algorithm\nThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.\n\n\n=== By design paradigm ===\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:\n\nBrute-force or exhaustive search\nThis is the naive method of trying every possible solution to see which is best.\nDivide and conquer\nA divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.\nSearch and enumeration\nMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.\nRandomized algorithm\nSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexity\nThis technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\nBack tracking\nIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\n\n\n=== Optimization problems ===\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\nLinear programming\nWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\nDynamic programming\nWhen a problem shows optimal substructures\u2014meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems\u2014and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd\u2013Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.\nThe greedy method\nA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\nThe heuristic method\nIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\n\n\n=== By field of study ===\n\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\n\n\n=== By complexity ===\n\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\n\nConstant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.\nLogarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.\nLinear time: if the time is proportional to the input size. E.g. the traverse of a list.\nPolynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.\nExponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\n\n\n== Continuous algorithms ==\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\n\nAn algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations\u2014such algorithms are studied in numerical analysis; or\nAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.\n\n\n== Legal issues ==\n\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\n\n== History: Development of the notion of \"algorithm\" ==\n\n\n=== Ancient Near East ===\nThe earliest  evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm. During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus circa 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).\n\n\n=== Discrete and distinguishable symbols ===\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16\u201341). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post\u2013Turing machine computations.\n\n\n=== Manipulation of symbols as \"place holders\" for numbers: algebra ===\nMuhammad ibn M\u016bs\u0101 al-Khw\u0101rizm\u012b, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms \"algorism\" and \"algorithm\" are derived from the name al-Khw\u0101rizm\u012b, while the term \"algebra\" is derived from the book Al-jabr. In Europe, the word \"algorithm\" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):\n\nA good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.\n\n\n=== Cryptographic algorithms ===\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.\n\n\n=== Mechanical contrivances with discrete states ===\nThe clock: Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"\u2014the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer\u2014Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator\u2014and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.\nLogical machines 1870 \u2013 Stanley Jevons' \"logical abacus\" and \"logical machine\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc] ...\". With this machine he could analyze a \"syllogism or any other simple logical argument\".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".Jacquard loom, Hollerith punch cards, telegraphy and telephony \u2013 the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.\nTelephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".Davis (2000) observes the particular importance of the electromechanical relay (with its two \"binary states\" open and closed):\n\nIt was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.\"\n\n\n=== Mathematics during the 19th century up to the mid-20th century ===\nSymbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888\u20131889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".But Heijenoort gives Frege (1879) this kudos: Frege's is \"perhaps the most important single work ever written in logic. ... in which we see a \" 'formula language', that is a lingua characterica, a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910\u20131913).\nThe paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902\u201303), and the Richard Paradox. The resultant considerations led to Kurt G\u00f6del's paper (1931)\u2014he specifically cites the paradox of the liar\u2014that completely reduces rules of recursion to numbers.\nEffective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's \u03bb-calculus a finely honed definition of \"general recursion\" from the work of G\u00f6del acting on suggestions of Jacques Herbrand (cf. G\u00f6del's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"\u2014in effect almost identical to Post's \"formulation\", J. Barkley Rosser's definition of \"effective method\" in terms of \"a machine\". S.C. Kleene's proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene's renaming his Thesis \"Church's Thesis\" and proposing \"Turing's Thesis\".\n\n\n=== Emil Post (1936) and Alan Turing (1936\u201337, 1939) ===\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\n\n\"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be\n\n\"a two-way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.\"One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes...\"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]\". See more at Post\u2013Turing machine\nAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'\". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.\nTuring\u2014his model of computation is now called a Turing machine\u2014begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\n\"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...\"The behavior of the computer at any moment is determined by the symbols which he is observing, and his \"state of mind\" at that moment. We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...\"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided.\"Turing's reduction yields the following:\n\n\"The simple operations must therefore include:\n\"(a) Changes of the symbol on one of the observed squares\n\"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\n\n\"(A) A possible change (a) of symbol together with a possible change of state of mind.\n\"(B) A possible change (b) of observed squares, together with a possible change of state of mind\"\"We may now construct a machine to do the work of this computer.\"A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\n\n\"A function is said to be \"effectively calculable\" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to G\u00f6del, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability \u2020 with effective calculability ... .\n\"\u2020 We shall use the expression \"computable function\" to mean a function calculable by a machine, and we let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions\".\n\n\n=== J.B. Rosser (1939) and S.C. Kleene (1943) ===\nJ. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):\n\n\"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.\" (Rosser 1939:225\u2013226)Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of \u03bb-definability, in particular Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and G\u00f6del and their use of recursion in particular G\u00f6del's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936\u201337) in their mechanism-models of computation.\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church\u2013Turing thesis. But he did this in the following context (boldface in original):\n\n\"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, \"yes\" or \"no,\" to the question, \"is the predicate value true?\"\" (Kleene 1943:273)\n\n\n=== History after 1950 ===\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church\u2013Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== Bibliography ==\n\n\n== Further reading ==\n\n\n== External links ==\n\"Algorithm\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nAlgorithms at Curlie\nWeisstein, Eric W. \"Algorithm\". MathWorld.\nDictionary of Algorithms and Data Structures \u2013 National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository \u2013 State University of New York at Stony Brook\nCollected Algorithms of the ACM \u2013 Association for Computing Machinery\nThe Stanford GraphBase \u2013 Stanford University"
This text should be understood on average by 25 year olds.
https://en.wikipedia.org/wiki/Automata_theory
Automata theory - Wikipedia
Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science. The word automata (the plural of automaton) comes from the Greek word \u03b1\u1f50\u03c4\u03cc\u03bc\u03b1\u03c4\u03b1, which means \"self-making\". An automaton (Automata in plural) is an abstract self-propelled computing device which follows a predetermined sequence of operations automatically. An automaton with a finite number of states is called a Finite Automaton (FA) or Finite State Machine (FSM).\n\nThe figure at right illustrates a finite-state machine, which belongs to a well-known type of automaton.  This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows).  As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the current state and the recent symbol as its inputs.\nAutomata theory is closely related to formal language theory. An automaton is a finite representation of a formal language that may be an infinite set. Automata are often classified by the class of formal languages they can recognize, typically illustrated by the Chomsky hierarchy, which describes the relations between various languages and kinds of formalized logics.\nAutomata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification.\n\n\n== Automata ==\nFollowing is an introductory definition of one type of automaton, which attempts to help one grasp the essential concepts involved in automata theory/theories.\n\n\n=== Very informal description ===\nAn automaton is a construct made of states designed to determine if the input should be accepted or rejected. It looks a lot like a basic board game where each space on the board represents a state. Each state has information about what to do when an input is received by the machine (again, rather like what to do when you land on the Go To Jail spot in a popular board game). As the machine receives a new input, it looks at the state and picks a new spot based on the information on what to do when it receives that input at that state. When there are no more inputs, the automaton stops and the space it is on when it completes determines whether the automaton accepts or rejects that particular set of inputs.\n\n\n=== Informal description ===\nAn automaton runs when it is given some sequence of inputs in discrete (individual) time steps or steps. An automaton processes one input picked from a set of symbols or letters, which is called an alphabet. The symbols received by the automaton as input at any step are a finite sequence of symbols called words. An automaton has a finite set of states. At each moment during a run of the automaton, the automaton is in one of its states. When the automaton receives new input it moves to another state (or transitions) based on a function that takes the current state and symbol as parameters. This function is called the transition function. The automaton reads the symbols of the input word one after another and transitions from state to state according to the transition function until the word is read completely. Once the input word has been read, the automaton is said to have stopped. The state at which the automaton stops is called the final state. Depending on the final state, it's said that the automaton either accepts or rejects an input word. There is a subset of states of the automaton, which is defined as the set of accepting states.  If the final state is an accepting state, then the automaton accepts the word. Otherwise, the word is rejected. The set of all the words accepted by an automaton is called the language recognized by the automaton.\nIn short, an automaton is a mathematical object that takes a word as input and decides whether to accept it or reject it. Since all computational problems are reducible into the accept/reject question on inputs, (all problem instances can be represented in a finite length of symbols),\nautomata theory plays a crucial role in computational theory.\n\n\n=== Formal definition ===\nAutomaton\n\n\n==== definition of finite state automata ====\nA deterministic finite automaton is represented formally by a 5-tuple <Q, \u03a3, \u03b4, q0, F>, where:\nQ is a finite set of states.\n\u03a3 is a finite set of symbols, called the alphabet of the automaton.\n\u03b4 is the transition function, that is, \u03b4: Q \u00d7 \u03a3 \u2192 Q.\nq0 is the start state, that is, the state of the automaton before any input has been processed, where q0\u2208 Q.\nF is a set of states of Q (i.e. F\u2286Q) called accept states.Input word\nAn automaton reads a finite string of symbols a1,a2,...., an , where ai \u2208 \u03a3, which is called an input word. The set of all words is denoted by \u03a3*.\nRun\nA sequence of states q0,q1,q2,...., qn, where qi \u2208 Q such that q0 is the start state and qi = \u03b4(qi-1,ai) for 0 < i \u2264 n, is a run of the automaton on an input word w = a1,a2,...., an \u2208 \u03a3*. In other words, at first the automaton is at the start state q0, and then the automaton reads symbols of the input word in sequence. When the automaton reads symbol ai it jumps to state qi = \u03b4(qi-1,ai). qn is said to be the final state of the run.Accepting word\nA word w \u2208 \u03a3* is accepted by the automaton if qn \u2208 F.Recognized language\nAn automaton can recognize a formal language. The language L \u2286 \u03a3* recognized by an automaton is the set of all the words that are accepted by the automaton.Recognizable languages\nThe recognizable languages are the set of languages that are recognized by some automaton. For the above definition of automata the recognizable languages are regular languages. For different definitions of automata, the recognizable languages are different.\n\n\n== Variant definitions of automata ==\nAutomata are defined to study useful machines under mathematical formalism. So, the definition of an automaton is open to variations according to the \"real world machine\", which we want to model using the automaton. People have studied many variations of automata. The most standard variant, which is described above, is called a deterministic finite automaton. The following are some popular variations in the definition of different components of automata.\n\nInputFinite input: An automaton that accepts only finite sequence of symbols. The above introductory definition only encompasses finite words.\nInfinite input: An automaton that accepts infinite words (\u03c9-words). Such automata are called \u03c9-automata.\nTree word input: The input may be a tree of symbols instead of sequence of symbols. In this case after reading each symbol, the automaton reads all the successor symbols in the input tree. It is said that the automaton makes one copy of itself for each successor and each such copy starts running on one of the successor symbols from the state according to the transition relation of the automaton. Such an automaton is called a tree automaton.\nInfinite tree input : The two extensions above can be combined, so the automaton reads a tree structure with (in)finite branches. Such an automaton is called an infinite tree automatonStatesFinite states: An automaton that contains only a finite number of states. The above introductory definition describes automata with finite numbers of states.\nInfinite states: An automaton that may not have a finite number of states, or even a countable number of states.  For example, the quantum finite automaton or topological automaton has uncountable infinity of states.\nStack memory: An automaton may also contain some extra memory in the form of a stack in which symbols can be pushed and popped. This kind of automaton is called a pushdown automatonTransition functionDeterministic: For a given current state and an input symbol, if an automaton can only jump to one and only one state then it is a deterministic automaton.\nNondeterministic: An automaton that, after reading an input symbol, may jump into any of a number of states, as licensed by its transition relation. Notice that the term transition function is replaced by transition relation: The automaton non-deterministically decides to jump into one of the allowed choices. Such automata are called nondeterministic automata.\nAlternation: This idea is quite similar to tree automaton but orthogonal. The automaton may run its multiple copies on the same next read symbol. Such automata are called alternating automata. Acceptance condition must satisfy all runs of such copies to accept the input.Acceptance conditionAcceptance of finite words: Same as described in the informal definition above.\nAcceptance of infinite words: an omega automaton cannot have final states, as infinite words never terminate. Rather, acceptance of the word is decided by looking at the infinite sequence of visited states during the run.\nProbabilistic acceptance: An automaton need not strictly accept or reject an input. It may accept the input with some probability between zero and one. For example, quantum finite automaton, geometric automaton and metric automaton have probabilistic acceptance.Different combinations of the above variations produce many classes of automaton.\nAutomata theory is a subject matter that studies properties of various types of automata. For example, the following questions are studied about a given type of automata.\n\nWhich class of formal languages is recognizable by some type of automata? (Recognizable languages)\nAre certain automata closed under union, intersection, or complementation of formal languages? (Closure properties)\nHow expressive is a type of automata in terms of recognizing a class of formal languages? And, their relative expressive power? (Language hierarchy)Automata theory also studies the existence or nonexistence of any effective algorithms to solve problems similar to the following list:\n\nDoes an automaton accept any input word? (Emptiness checking)\nIs it possible to transform a given non-deterministic automaton into deterministic automaton without changing the recognizable language? (Determinization)\nFor a given formal language, what is the smallest automaton that recognizes it? (Minimization)\n\n\n== Classes of automata ==\nThe following is an incomplete list of types of automata.\n\n\n=== Discrete, continuous, and hybrid automata ===\nNormally automata theory describes the states of abstract machines but there are discrete automata, analog automata or continuous automata, or hybrid discrete-continuous automata, which use digital data, analog data or continuous time, or digital and analog data, respectively.\n\n\n== Hierarchy in terms of powers ==\nThe following is an incomplete hierarchy in terms of powers of different types of virtual machines. The hierarchy reflects the nested categories of languages the machines are able to accept.\n\n\n== Applications ==\nEach model in automata theory plays important roles in several applied areas. Finite automata are used in text processing, compilers, and hardware design. Context-free grammar (CFGs) are used in programming languages and artificial intelligence. Originally, CFGs were used in the study of the human languages. Cellular automata are used in the field of biology, the most common example being John Conway's Game of Life. Some other examples which could be explained using automata theory in biology include mollusk and pine cones growth and pigmentation patterns. Going further, a theory suggesting that the whole universe is computed by some sort of a discrete automaton, is advocated by some scientists. The idea originated in the work of Konrad Zuse, and was popularized in America by Edward Fredkin. Automata also appear in the theory of finite fields: the set of irreducible polynomials which can be written as composition of degree two polynomials is in fact a regular language.\nAnother problem for which automata can be used is the induction of regular languages.\n\n\n== Automata simulators ==\nAutomata simulators are pedagogical tools used to teach, learn and research automata theory. An automata simulator takes as input the description of an automaton and then simulates its working for an arbitrary input string. The description of the automaton can be entered in several ways. An automaton can be defined in a symbolic language  or its specification may be entered in a predesigned form or its transition diagram may be drawn by clicking and dragging the mouse. Well known automata simulators include Turing's World, JFLAP, VAS, TAGS and SimStudio.\n\n\n== Connection to category theory ==\nOne can define several distinct categories of automata following the automata classification into different types described in the previous section. The mathematical category of deterministic automata, sequential machines or sequential automata, and Turing machines with automata homomorphisms defining the arrows between automata is a Cartesian closed category, it has both categorical limits and colimits. An automata homomorphism maps a quintuple of an automaton Ai onto the quintuple of another automaton \n Aj. Automata homomorphisms can also be considered as automata transformations or as semigroup homomorphisms, when the state space, S, of the automaton is defined as a semigroup Sg. Monoids are also considered as a suitable setting for automata in monoidal categories.\nCategories of variable automataOne could also define a variable automaton, in the sense of Norbert Wiener in his book on The Human Use of Human Beings via the endomorphisms \n  \n    \n      \n        \n          A\n          \n            i\n          \n        \n        \u2192\n        \n          A\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle A_{i}\\to A_{i}}\n  . Then, one can show that such variable automata homomorphisms form a mathematical group. In the case of non-deterministic, or other complex kinds of automata, the latter set of endomorphisms may become, however, a variable automaton groupoid. Therefore, in the most general case, categories of variable automata of any kind are categories of groupoids or groupoid categories. Moreover, the category of reversible automata is then a \n2-category, and also a subcategory of the 2-category of groupoids, or the groupoid category.\n\n\n== History ==\nThe automata theory was developed in the mid-20th century in connection with finite automata.\n\n\n== See also ==\nBoolean differential calculus\n\n\n== References ==\n\n\n== Further reading ==\nJohn E. Hopcroft, Rajeev Motwani, Jeffrey D. Ullman (2000). Introduction to Automata Theory, Languages, and Computation (2nd ed.). Pearson Education. ISBN 978-0-201-44124-6.CS1 maint: uses authors parameter (link)\nMichael Sipser (1997). Introduction to the Theory of Computation. PWS Publishing. ISBN 978-0-534-94728-6. Part One: Automata and Languages, chapters 1\u20132, pp. 29\u2013122. Section 4.1: Decidable Languages, pp. 152\u2013159. Section 5.1: Undecidable Problems from Language Theory, pp. 172\u2013183.\nElaine Rich (2008). Automata, Computability and Complexity: Theory and Applications. Pearson. ISBN 978-0-13-228806-4.\nSalomaa, Arto (1985). Computation and automata. Encyclopedia of Mathematics and Its Applications. 25. Cambridge University Press. ISBN 978-0-521-30245-6. Zbl 0565.68046.\nAnderson, James A. (2006). Automata theory with modern applications. With contributions by Tom Head. Cambridge: Cambridge University Press. ISBN 978-0-521-61324-8. Zbl 1127.68049.\nConway, J.H. (1971). Regular algebra and finite machines. Chapman and Hall Mathematics Series. London: Chapman & Hall. Zbl 0231.94041.\nJohn M. Howie (1991) Automata and Languages, Clarendon Press ISBN 0-19-853424-8 MR1254435\nSakarovitch, Jacques (2009). Elements of automata theory. Translated from the French by Reuben Thomas. Cambridge University Press. ISBN 978-0-521-84425-3. Zbl 1188.68177.\nJames P. Schmeiser, David T. Barnard (1995). Producing a top-down parse order with bottom-up parsing. Elsevier North-Holland.CS1 maint: uses authors parameter (link)\nIgor Aleksander, F.Keith Hanna (1975). Automata Theory : An Engineering Approach. New York: Crane Russak. ISBN 978-0-8448-0657-0.CS1 maint: uses authors parameter (link)\nMarvin Minsky (1967). Computation : Finite and infinite machines. Princeton, N.J.: Prentice Hall.\nJohn C. Martin (2011). Introduction to Languages and The Theory of Computation. New York, NY 10020: McGraw Hill. ISBN 978-0-07-319146-1.CS1 maint: location (link)\n\n\n== External links ==\nVisual Automata Simulator, A tool for simulating, visualizing and transforming finite state automata and Turing Machines, by Jean Bovet\nJFLAP\ndk.brics.automaton\nlibfa"
This text should be understood on average by 18 year olds.
https://en.wikipedia.org/wiki/Theory_of_computation
Theory of computation - Wikipedia
In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\".In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation.  There are several models in use, but the most commonly examined is the Turing machine. Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible \"reasonable\" model of computation (see Church\u2013Turing thesis). It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.\n\n\n== History ==\nThe theory of computation can be considered the creation of models of all kinds in the field of computer science. Therefore, mathematics and logic are used. In the last century it became an independent academic discipline and was separated from mathematics.\nSome pioneers of the theory of computation were Ramon Llull, Alonzo Church, Kurt G\u00f6del, Alan Turing, Stephen Kleene, R\u00f3zsa P\u00e9ter, John von Neumann and Claude Shannon.\n\n\n== Branches ==\n\n\n=== Automata theory ===\n\nAutomata theory is the study of abstract machines (or more appropriately, abstract 'mathematical' machines or systems) and the computational problems that can be solved using these machines. These abstract machines are called automata. Automata comes from the Greek word (\u0391\u03c5\u03c4\u03cc\u03bc\u03b1\u03c4\u03b1) which means that something is doing something by itself.\nAutomata theory is also closely related to formal language theory, as the automata are often classified by the class of formal languages they are able to recognize. An automaton can be a finite representation of a formal language that may be an infinite set. Automata are used as theoretical models for computing machines, and are used for proofs about computability.\n\n\n==== Formal Language theory ====\n\nLanguage theory is a branch of mathematics concerned with describing languages as a set of operations over an alphabet. It is closely linked with automata theory, as automata are used to generate and recognize formal languages. There are several classes of formal languages, each allowing more complex language specification than the one before it, i.e. Chomsky hierarchy, and each corresponding to a class of automata which recognizes it. Because automata are used as models for computation, formal languages are the preferred mode of specification for any problem that must be computed.\n\n\n=== Computability theory ===\n\nComputability theory deals primarily with the question of the extent to which a problem is solvable on a computer. The statement that the halting problem cannot be solved by a Turing machine is one of the most important results in computability theory, as it is an example of a concrete problem that is both easy to formulate and impossible to solve using a Turing machine.  Much of computability theory builds on the halting problem result.\nAnother important step in computability theory was Rice's theorem, which states that for all non-trivial properties of partial functions, it is undecidable whether a Turing machine computes a partial function with that property.Computability theory is closely related to the branch of mathematical logic called recursion theory, which removes the restriction of studying only models of computation which are reducible to the Turing model.  Many mathematicians and computational theorists who study recursion theory will refer to it as computability theory.\n\n\n=== Computational complexity theory ===\n\nComplexity theory considers not only whether a problem can be solved at all on a computer, but also how efficiently the problem can be solved.  Two major aspects are considered: time complexity and space complexity, which are respectively how many steps does it take to perform a computation, and how much memory is required to perform that computation.\nIn order to analyze how much time and space a given algorithm requires, computer scientists express the time or space required to solve the problem as a function of the size of the input problem.  For example, finding a particular number in a long list of numbers becomes harder as the list of numbers grows larger.  If we say there are n numbers in the list, then if the list is not sorted or indexed in any way we may have to look at every number in order to find the number we're seeking.  We thus say that  in order to solve this problem, the computer needs to perform a number of steps that grows linearly in the size of the problem.\nTo simplify this problem, computer scientists have adopted Big O notation, which allows functions to be compared in a way that ensures that particular aspects of a machine's construction do not need to be considered, but rather only the asymptotic behavior as problems become large.  So in our previous example, we might say that the problem requires \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   steps to solve.\nPerhaps the most important open problem in all of computer science is the question of whether a certain broad class of problems denoted NP can be solved efficiently. This is discussed further at Complexity classes P and NP, and P versus NP problem is one of the seven Millennium Prize Problems stated by the Clay Mathematics Institute in 2000. The Official Problem Description was given by Turing Award winner Stephen Cook.\n\n\n== Models of computation ==\n\nAside from a Turing machine, other equivalent (See: Church\u2013Turing thesis) models of computation are in use.\n\nLambda calculus\nA computation consists of an initial lambda expression (or two if you want to separate the function and its input) plus a finite sequence of lambda terms, each deduced from the preceding term by one application of Beta reduction.\nCombinatory logic\nis a concept which has many similarities to \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  -calculus, but also important differences exist (e.g. fixed point combinator Y has normal form in combinatory logic but not in \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  -calculus). Combinatory logic was developed with great ambitions: understanding the nature of paradoxes, making foundations of mathematics more economic (conceptually), eliminating the notion of variables (thus clarifying their role in mathematics).\u03bc-recursive functions\na computation consists of a mu-recursive function, i.e. its defining sequence, any input value(s) and a sequence of recursive functions appearing in the defining sequence with inputs and outputs.  Thus, if in the defining sequence of a recursive function \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   the functions \n  \n    \n      \n        g\n        (\n        x\n        )\n      \n    \n    {\\displaystyle g(x)}\n   and \n  \n    \n      \n        h\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle h(x,y)}\n   appear, then terms of the form 'g(5)=7' or 'h(3,2)=10' might appear.  Each entry in this sequence needs to be an application of a basic function or follow from the entries above by using composition, primitive recursion or \u03bc recursion.  For instance if \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        h\n        (\n        x\n        ,\n        g\n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle f(x)=h(x,g(x))}\n  , then for 'f(5)=3' to appear, terms like 'g(5)=6' and 'h(5,6)=3' must occur above.  The computation terminates only if the final term gives the value of the recursive function applied to the inputs.Markov algorithm\na string rewriting system that uses grammar-like rules to operate on strings of symbols.Register machine\nis a theoretically interesting idealization of a computer. There are several variants. In most of them, each register can hold a natural number (of unlimited size), and the instructions are simple (and few in number), e.g. only decrementation (combined with conditional jump) and incrementation exist (and halting). The lack of the infinite (or dynamically growing) external store (seen at Turing machines) can be understood by replacing its role with G\u00f6del numbering techniques: the fact that each register holds a natural number allows the possibility of representing a complicated thing (e.g. a sequence, or a matrix etc.) by an appropriate huge natural number \u2014 unambiguity of both representation and interpretation can be established by number theoretical  foundations of these techniques.In addition to the general computational models, some simpler computational models are useful for special, restricted applications.  Regular expressions, for example,  specify string patterns in many contexts, from office productivity software to programming languages. Another formalism mathematically equivalent to regular expressions, Finite automata are used in circuit design and in some kinds of problem-solving. Context-free grammars  specify programming language syntax.  Non-deterministic pushdown automata are another formalism equivalent to context-free grammars. Primitive recursive functions are a defined subclass of the recursive functions.\nDifferent models of computation have the ability to do different tasks. One way to measure the power of a computational model is to study the class of formal languages that the model can generate; in such a way to the Chomsky hierarchy of languages is obtained.\n\n\n== References ==\n\n\n== Further reading ==\nTextbooks aimed at computer scientists(There are many textbooks in this area; this list is by necessity incomplete.)\n\nHopcroft, John E., and Jeffrey D. Ullman (2006). Introduction to Automata Theory, Languages, and Computation. 3rd ed  Reading, MA: Addison-Wesley. ISBN 978-0-321-45536-9 One of the standard references in the field.\nLinz P. An introduction to formal language and automata. Narosa Publishing. ISBN 9788173197819.\nMichael Sipser (2013). Introduction to the Theory of Computation (3rd ed.). Cengage Learning. ISBN 978-1-133-18779-0.\nEitan Gurari (1989). An Introduction to the Theory of Computation. Computer Science Press. ISBN 0-7167-8182-4. Archived from the original on 2007-01-07.\nHein, James L. (1996) Theory of Computation.  Sudbury, MA: Jones & Bartlett.  ISBN 978-0-86720-497-1 A gentle introduction to the field, appropriate for second-year undergraduate computer science students.\nTaylor, R. Gregory (1998). Models of Computation and Formal Languages.  New York: Oxford University Press.  ISBN 978-0-19-510983-2  An unusually readable textbook, appropriate for upper-level undergraduates or beginning graduate students.\nLewis, F. D. (2007). Essentials of theoretical computer science A textbook covering the topics of formal languages, automata and grammars. The emphasis appears to be on presenting an overview of the results and their applications rather than providing proofs of the results.\nMartin Davis, Ron Sigal, Elaine J. Weyuker, Computability, complexity, and languages: fundamentals of theoretical computer science, 2nd ed., Academic Press, 1994, ISBN 0-12-206382-1. Covers a wider range of topics than most other introductory books, including program semantics and quantification theory. Aimed at graduate students.Books on computability theory from the (wider) mathematical perspectiveHartley Rogers, Jr (1987). Theory of Recursive Functions and Effective Computability, MIT Press. ISBN 0-262-68052-1\nS. Barry Cooper (2004). Computability Theory. Chapman and Hall/CRC. ISBN 1-58488-237-9..\nCarl H. Smith, A recursive introduction to the theory of computation, Springer, 1994, ISBN 0-387-94332-3. A shorter textbook suitable for graduate students in Computer Science.Historical perspectiveRichard L. Epstein and Walter A. Carnielli (2000). Computability: Computable Functions, Logic, and the Foundations of Mathematics, with Computability: A Timeline (2nd ed.). Wadsworth/Thomson Learning. ISBN 0-534-54644-7..\n\n\n== External links ==\nTheory of Computation at MIT\nTheory of Computation at Harvard\nComputability Logic - A theory of interactive computation. The main web source on this subject."
This text should be understood on average by 24 year olds.
